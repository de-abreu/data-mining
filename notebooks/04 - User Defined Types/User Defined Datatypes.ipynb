{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb25ad57-ce0a-4d7a-a867-a6ae947f0be8",
   "metadata": {},
   "source": [
    "## Autores\n",
    "\n",
    "| Nome | nUSP |\n",
    "| :--- | :--- |\n",
    "| Guilherme de Abreu Barreto | 12543033 |\n",
    "| Lucas Eduardo Gulka Pulcinelli | 12547336 |\n",
    "| Vinicio Yusuke Hayashibara | 13642797 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "804ab7b6-3450-43c2-afab-c3b6ace19568",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_DATABASE = \"postgres\"\n",
    "CENSO_DATABASE = \"censo2022\"\n",
    "USER = \"postgres\"\n",
    "PASSWORD = \"postgres\"\n",
    "HOST = \"localhost\"\n",
    "PORT = 5432\n",
    "URI = f\"postgresql+psycopg2://{USER}:{PASSWORD}@{HOST}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36e35410-453c-4bf3-9434-f6eaf4e2ab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from enum import Enum\n",
    "from math import sqrt\n",
    "from sqlalchemy import (\n",
    "    BigInteger,\n",
    "    Float,\n",
    "    Integer,\n",
    "    Index,\n",
    "    String,\n",
    "    CheckConstraint as constraint,\n",
    "    UniqueConstraint as unique,\n",
    "    PrimaryKeyConstraint as pkc,\n",
    "    ForeignKeyConstraint as fkc,\n",
    "    ForeignKey as fk,\n",
    "    JSON,\n",
    "    cast,\n",
    "    create_engine,\n",
    "    insert,\n",
    "    text,\n",
    "    func,\n",
    ")\n",
    "from sqlalchemy.orm import (\n",
    "    Mapped,\n",
    "    Session,\n",
    "    composite,\n",
    "    declarative_base,\n",
    "    relationship,\n",
    "    sessionmaker,\n",
    "    mapped_column as column,\n",
    ")\n",
    "from sqlalchemy.dialects.postgresql import ENUM\n",
    "from sqlalchemy.ext.hybrid import hybrid_method, hybrid_property\n",
    "from sqlalchemy.sql.schema import CheckConstraint\n",
    "from typing import Optional, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9363de9-0448-4cef-87cc-84107c3cc933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backref(back_populates: str) -> Mapped[Any]:\n",
    "    return relationship(back_populates=back_populates)\n",
    "\n",
    "\n",
    "def childOf(back_populates: str) -> Mapped[Any]:\n",
    "    return relationship(\n",
    "        back_populates=back_populates,\n",
    "        cascade=\"all, delete-orphan\",\n",
    "    )\n",
    "\n",
    "def digits(name:str) -> CheckConstraint:\n",
    "    return constraint(\"id ~ '^[0-9]+$'\", name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28f60461-1951-4e40-a91b-a0665447ae9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Households:\n",
    "    def __init__(self, urban: int, rural: int) -> None:\n",
    "        self.urban = urban\n",
    "        self.rural = rural\n",
    "\n",
    "    def __composite_values__(self) -> tuple[int, ...]:\n",
    "        return (self.urban, self.rural)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, Households) and \\\n",
    "               other.urban == self.urban and \\\n",
    "               other.rural == self.rural\n",
    "        \n",
    "    @hybrid_property\n",
    "    def total(self):\n",
    "        \"\"\"Python-side property for total households.\"\"\"\n",
    "        return self.urban + self.rural\n",
    "\n",
    "    @total.expression\n",
    "    def total(cls):\n",
    "        \"\"\"SQL-side expression for querying total households.\"\"\"\n",
    "        return cls.urban + cls.rural\n",
    "\n",
    "class Coordinate:\n",
    "    \"\"\"\n",
    "    A geographic coordinate point with longitude (lon) and latitude (lat) components.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, longitude: float, latitude: float) -> None:\n",
    "        self.longitude = longitude\n",
    "        self.latitude = latitude\n",
    "\n",
    "    @property\n",
    "    def longitude(self) -> float:\n",
    "        return self._longitude\n",
    "\n",
    "    @longitude.setter\n",
    "    def longitude(self, value: float) -> None:\n",
    "        if not (-180 <= value <= 180):\n",
    "            raise ValueError(f\"Longitude must be between -180 and 180 degrees, got {value}\")\n",
    "        self._longitude = value\n",
    "\n",
    "    @property\n",
    "    def latitude(self) -> float:\n",
    "        return self._latitude\n",
    "\n",
    "    @latitude.setter\n",
    "    def latitude(self, value: float) -> None:\n",
    "        if not (-90 <= value <= 90):\n",
    "            raise ValueError(f\"Latitude must be between -90 and 90 degrees, got {value}\")\n",
    "        self._latitude = value\n",
    "\n",
    "    def __composite_values__(self) -> tuple[float, ...]:\n",
    "        return (self.longitude, self.latitude)\n",
    "\n",
    "    def __eq__(self, other: \"Coordinate\") -> bool:\n",
    "        return isinstance(other, Coordinate) and \\\n",
    "               other.longitude == self.longitude and \\\n",
    "               other.latitude == self.latitude\n",
    "        \n",
    "    def __ne__ (self, other: \"Coordinate\") -> bool:\n",
    "        return not self.__eq__(other)\n",
    "\n",
    "    @hybrid_method\n",
    "    def distance(\n",
    "        self,\n",
    "        other: \"Coordinate\",\n",
    "        metric: str = 'euclidean'\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Calculate distance to another coordinate.\n",
    "        \n",
    "        Args:\n",
    "            other: Coordinate instance\n",
    "            metric: 'euclidean' or 'manhattan'\n",
    "        \n",
    "        Returns:\n",
    "            Distance between coordinates\n",
    "        \"\"\"\n",
    "        x = self.longitude - other.longitude\n",
    "        y = self.latitude - other.latitude\n",
    "        match metric:\n",
    "            case 'euclidean':\n",
    "                return sqrt(x**2 + y**2)\n",
    "            case 'manhattan':\n",
    "                return abs(x) + abs(y)\n",
    "            case _:\n",
    "                raise ValueError(\"Metric must be 'euclidean' or 'manhattan'\")\n",
    "\n",
    "    @distance.expression\n",
    "    def distance(\n",
    "        cls,\n",
    "        other_lon: float,\n",
    "        other_lat: float,\n",
    "        metric: str = 'euclidean'\n",
    "    ):\n",
    "        match metric:\n",
    "            case 'euclidean':\n",
    "                return func.sqrt(\n",
    "                    (cls.x - other_x) * (cls.x - other_x) +\n",
    "                    (cls.y - other_y) * (cls.y - other_y)\n",
    "                )\n",
    "            case 'manhattan':\n",
    "                return func.abs(cls.x - other_x) + func.abs(cls.y - other_y)\n",
    "            case _:\n",
    "                raise ValueError(\"Metric must be 'euclidean' or 'manhattan'\")\n",
    "\n",
    "\n",
    "class Biomes:\n",
    "    default: dict[str, float] = {\n",
    "        biome: 0.0 for biome in [\n",
    "            'amazon_rainforest',\n",
    "            'atlantic_forest',\n",
    "            'caatinga',\n",
    "            'cerrado',\n",
    "            'pantanal',\n",
    "            'pampas'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        self.distribution = kargs\n",
    "\n",
    "    def __composite_values__(self) -> tuple[float, ...]:\n",
    "        return tuple(getattr(self, biome) for biome in self.default.keys())\n",
    "\n",
    "    @property\n",
    "    def distribution(self) -> dict[str, float]:\n",
    "        return {biome: getattr(self, biome) for biome in self.default.keys()}\n",
    "\n",
    "\n",
    "    @distribution.setter\n",
    "    def distribution(self, values: dict[str, float]) -> None:\n",
    "        merged_values = {**self.default, **values}\n",
    "\n",
    "        # Validation\n",
    "        invalid_keys = set(merged_values.keys()) - set(self.default.keys())\n",
    "        if invalid_keys:\n",
    "            raise ValueError(f\"Invalid biome types: {invalid_keys}. Valid types are: {list(self.default.keys())}\")\n",
    "        total = sum(merged_values.values())\n",
    "        if 99.9 <= total <= 100.1:\n",
    "            raise ValueError(f\"Invalid biome distribution, totalling {total:.1f}%\")\n",
    "        self._distribution = merged_values\n",
    "\n",
    "        for biome_type, value in merged_values.items():\n",
    "            setattr(self, biome_type, value)\n",
    "\n",
    "    @property\n",
    "    def total(self) -> float:\n",
    "        return sum(getattr(self, biome) for biome in self.default.keys)\n",
    "\n",
    "    @classmethod\n",
    "    def toList(cls) -> list[str]:\n",
    "        return list(cls.default.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a55a5be-368e-4285-b77b-da320d68b487",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_enum = ENUM(*[\n",
    "    \"AC\", \"AL\", \"AP\", \"AM\", \"BA\", \"CE\", \"DF\", \"ES\", \"GO\", \"MA\", \n",
    "    \"MT\", \"MS\", \"MG\", \"PA\", \"PB\", \"PR\", \"PE\", \"PI\", \"RJ\", \"RN\", \n",
    "    \"RS\", \"RO\", \"RR\", \"SC\", \"SP\", \"SE\", \"TO\"\n",
    "], name=\"state_enum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "198f174f-0553-4092-8f21-9d1f041c0ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "class Region(Base):\n",
    "    __tablename__: str = \"regions\"\n",
    "\n",
    "    # Attributes\n",
    "    id: Mapped[str] = column(\n",
    "        String(1),\n",
    "        digits(\"ck_region_id\"),\n",
    "        primary_key=True\n",
    "    )\n",
    "    name: Mapped[str] = column(unique=True)\n",
    "\n",
    "    # Relationships\n",
    "    states: Mapped[list[\"State\"]] = childOf('region')\n",
    "\n",
    "\n",
    "class State(Base):\n",
    "    __tablename__: str = \"states\"\n",
    "\n",
    "    # Attributes\n",
    "    id: Mapped[str] = column(String(1), digits(\"ck_state_id\"))\n",
    "    name: Mapped[str] = column(unique=True)\n",
    "    uf: Mapped[str] = column(state_enum, unique=True)\n",
    "    location: Mapped[Coordinate] = composite(\n",
    "        column(\"longitude\", Float),\n",
    "        column(\"latitude\", Float)\n",
    "    )\n",
    "    area: Mapped[float]\n",
    "    biome_distribution: Mapped[Biomes] = composite(\n",
    "        *[column(biome, Float) for biome in Biomes.toList()]\n",
    "    )\n",
    "\n",
    "    # Foreign keys\n",
    "    region_id: Mapped[str] = column(fk(\"regions.id\"))\n",
    "\n",
    "    # Relationships\n",
    "    region: Mapped[\"Region\"] = backref(\"states\")\n",
    "    cities: Mapped[list[\"City\"]] = childOf(\"state\")\n",
    "\n",
    "    __table_args__: tuple[pkc, unique,] = (\n",
    "        pkc(\"region_id\", \"id\"),\n",
    "        unique('longitude', 'latitude', name='uq_state_location'),\n",
    "    )\n",
    "\n",
    "\n",
    "class City(Base):\n",
    "    __tablename__: str = \"cities\"\n",
    "\n",
    "    # Attributes\n",
    "    id: Mapped[str] = column(String(5), digits(\"ck_city_id\"))\n",
    "    name: Mapped[str]\n",
    "    is_capital: Mapped[bool] = column(default=False, server_default='false')\n",
    "    location: Mapped[Coordinate] = composite(\n",
    "        column(\"longitude\", Float),\n",
    "        column(\"latitude\", Float)\n",
    "    )\n",
    "    ddd: Mapped[str] = column(String(2), digits(\"ck_city_ddd\"))\n",
    "    households: Mapped[Households] = composite(\n",
    "        column(\"urban\", Integer),\n",
    "        column(\"rural\", Integer)\n",
    "    )\n",
    "    population_race: Mapped[dict] = column(JSON)\n",
    "    population_education: Mapped[dict] = column(JSON)\n",
    "    \n",
    "\n",
    "    # Foreign keys\n",
    "    timezone_name: Mapped[str] = column(fk(\"timezones.name\"))\n",
    "    region_id: Mapped[str]\n",
    "    state_id: Mapped[str]\n",
    "\n",
    "    # Relationships\n",
    "    timezone: Mapped[\"Timezone\"] = backref(\"cities\")\n",
    "    state: Mapped[\"State\"] = backref(\"cities\")\n",
    "\n",
    "    @hybrid_property\n",
    "    def ibge_code(self) -> str:\n",
    "        \"\"\"Python-side property to get the IBGE code.\"\"\"\n",
    "        return self.region_id + self.state_id + self.id\n",
    "\n",
    "    @ibge_code.expression\n",
    "    def ibge_code(cls):\n",
    "        \"\"\"SQL-side expression for querying.\"\"\"\n",
    "        return cast(\n",
    "            func.concat(\n",
    "                # Join to State to get the region ID\n",
    "                cast(cls.region_id, String),\n",
    "                cast(cls.state_id, String),\n",
    "                cast(cls.id, String)\n",
    "            ),\n",
    "            BigInteger\n",
    "        )\n",
    "\n",
    "    __table_args__: tuple[pkc, fkc, unique, ] = (\n",
    "        pkc(\"region_id\", \"state_id\", \"id\"),\n",
    "        fkc(\n",
    "            ['region_id', 'state_id'],\n",
    "            ['states.region_id', 'states.id'],\n",
    "            name='fk_region_composite'\n",
    "        ),\n",
    "        unique(\"longitude\", \"latitude\", name=\"uq_city_location\"),\n",
    "        Index(\n",
    "            'state_capitals_index'\n",
    "            'region_id', 'state_id',\n",
    "            postgresql_where=text('is_capital'),\n",
    "            unique=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "class Timezone(Base):\n",
    "    __tablename__: str = \"timezones\"\n",
    "\n",
    "    # Attributes\n",
    "    name: Mapped[str] = column(primary_key=True)\n",
    "    utc_offset: Mapped[int]\n",
    "\n",
    "    # Relationships\n",
    "    cities: Mapped[list[\"City\"]] = backref('timezone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b7d8296-2d1b-4358-92f0-b1a304f6875d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-19 20:54:24,954 INFO sqlalchemy.engine.Engine select pg_catalog.version()\n",
      "2025-09-19 20:54:24,955 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2025-09-19 20:54:24,957 INFO sqlalchemy.engine.Engine select current_schema()\n",
      "2025-09-19 20:54:24,958 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2025-09-19 20:54:24,960 INFO sqlalchemy.engine.Engine show standard_conforming_strings\n",
      "2025-09-19 20:54:24,961 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2025-09-19 20:54:24,965 INFO sqlalchemy.engine.Engine BEGIN (implicit; DBAPI should not BEGIN due to autocommit mode)\n",
      "2025-09-19 20:54:24,966 INFO sqlalchemy.engine.Engine CREATE DATABASE censo2022\n",
      "2025-09-19 20:54:24,967 INFO sqlalchemy.engine.Engine [generated in 0.00229s] {}\n",
      "2025-09-19 20:54:25,014 INFO sqlalchemy.engine.Engine ROLLBACK using DBAPI connection.rollback(), DBAPI should ignore due to autocommit mode\n"
     ]
    }
   ],
   "source": [
    "engine = create_engine(URI + DEFAULT_DATABASE, echo=True)\n",
    "\n",
    "with engine.connect().execution_options(isolation_level=\"AUTOCOMMIT\") as conn:\n",
    "    conn.execute(text(f\"CREATE DATABASE {CENSO_DATABASE}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07a4df84-58fa-458a-a1a3-e5707bad84c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-19 21:19:32,468 INFO sqlalchemy.engine.Engine select pg_catalog.version()\n",
      "2025-09-19 21:19:32,469 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2025-09-19 21:19:32,471 INFO sqlalchemy.engine.Engine select current_schema()\n",
      "2025-09-19 21:19:32,471 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2025-09-19 21:19:32,473 INFO sqlalchemy.engine.Engine show standard_conforming_strings\n",
      "2025-09-19 21:19:32,474 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2025-09-19 21:19:32,477 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-09-19 21:19:32,480 INFO sqlalchemy.engine.Engine SELECT pg_catalog.pg_class.relname \n",
      "FROM pg_catalog.pg_class JOIN pg_catalog.pg_namespace ON pg_catalog.pg_namespace.oid = pg_catalog.pg_class.relnamespace \n",
      "WHERE pg_catalog.pg_class.relname = %(table_name)s AND pg_catalog.pg_class.relkind = ANY (ARRAY[%(param_1)s, %(param_2)s, %(param_3)s, %(param_4)s, %(param_5)s]) AND pg_catalog.pg_table_is_visible(pg_catalog.pg_class.oid) AND pg_catalog.pg_namespace.nspname != %(nspname_1)s\n",
      "2025-09-19 21:19:32,480 INFO sqlalchemy.engine.Engine [generated in 0.00093s] {'table_name': 'regions', 'param_1': 'r', 'param_2': 'p', 'param_3': 'f', 'param_4': 'v', 'param_5': 'm', 'nspname_1': 'pg_catalog'}\n",
      "2025-09-19 21:19:32,483 INFO sqlalchemy.engine.Engine SELECT pg_catalog.pg_class.relname \n",
      "FROM pg_catalog.pg_class JOIN pg_catalog.pg_namespace ON pg_catalog.pg_namespace.oid = pg_catalog.pg_class.relnamespace \n",
      "WHERE pg_catalog.pg_class.relname = %(table_name)s AND pg_catalog.pg_class.relkind = ANY (ARRAY[%(param_1)s, %(param_2)s, %(param_3)s, %(param_4)s, %(param_5)s]) AND pg_catalog.pg_table_is_visible(pg_catalog.pg_class.oid) AND pg_catalog.pg_namespace.nspname != %(nspname_1)s\n",
      "2025-09-19 21:19:32,484 INFO sqlalchemy.engine.Engine [cached since 0.004177s ago] {'table_name': 'states', 'param_1': 'r', 'param_2': 'p', 'param_3': 'f', 'param_4': 'v', 'param_5': 'm', 'nspname_1': 'pg_catalog'}\n",
      "2025-09-19 21:19:32,485 INFO sqlalchemy.engine.Engine SELECT pg_catalog.pg_class.relname \n",
      "FROM pg_catalog.pg_class JOIN pg_catalog.pg_namespace ON pg_catalog.pg_namespace.oid = pg_catalog.pg_class.relnamespace \n",
      "WHERE pg_catalog.pg_class.relname = %(table_name)s AND pg_catalog.pg_class.relkind = ANY (ARRAY[%(param_1)s, %(param_2)s, %(param_3)s, %(param_4)s, %(param_5)s]) AND pg_catalog.pg_table_is_visible(pg_catalog.pg_class.oid) AND pg_catalog.pg_namespace.nspname != %(nspname_1)s\n",
      "2025-09-19 21:19:32,486 INFO sqlalchemy.engine.Engine [cached since 0.006565s ago] {'table_name': 'cities', 'param_1': 'r', 'param_2': 'p', 'param_3': 'f', 'param_4': 'v', 'param_5': 'm', 'nspname_1': 'pg_catalog'}\n",
      "2025-09-19 21:19:32,488 INFO sqlalchemy.engine.Engine SELECT pg_catalog.pg_class.relname \n",
      "FROM pg_catalog.pg_class JOIN pg_catalog.pg_namespace ON pg_catalog.pg_namespace.oid = pg_catalog.pg_class.relnamespace \n",
      "WHERE pg_catalog.pg_class.relname = %(table_name)s AND pg_catalog.pg_class.relkind = ANY (ARRAY[%(param_1)s, %(param_2)s, %(param_3)s, %(param_4)s, %(param_5)s]) AND pg_catalog.pg_table_is_visible(pg_catalog.pg_class.oid) AND pg_catalog.pg_namespace.nspname != %(nspname_1)s\n",
      "2025-09-19 21:19:32,489 INFO sqlalchemy.engine.Engine [cached since 0.009403s ago] {'table_name': 'timezones', 'param_1': 'r', 'param_2': 'p', 'param_3': 'f', 'param_4': 'v', 'param_5': 'm', 'nspname_1': 'pg_catalog'}\n",
      "2025-09-19 21:19:32,494 INFO sqlalchemy.engine.Engine SELECT pg_catalog.pg_type.typname \n",
      "FROM pg_catalog.pg_type JOIN pg_catalog.pg_namespace ON pg_catalog.pg_namespace.oid = pg_catalog.pg_type.typnamespace \n",
      "WHERE pg_catalog.pg_type.typname = %(typname_1)s AND pg_catalog.pg_type_is_visible(pg_catalog.pg_type.oid) AND pg_catalog.pg_namespace.nspname != %(nspname_1)s\n",
      "2025-09-19 21:19:32,495 INFO sqlalchemy.engine.Engine [generated in 0.00223s] {'typname_1': 'state_enum', 'nspname_1': 'pg_catalog'}\n",
      "2025-09-19 21:19:32,497 INFO sqlalchemy.engine.Engine COMMIT\n"
     ]
    }
   ],
   "source": [
    "engine = create_engine(URI + CENSO_DATABASE, echo=True)\n",
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52cf92cf-898b-4989-a0a7-438390d5c77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-19 21:24:47,102 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-09-19 21:24:47,105 INFO sqlalchemy.engine.Engine INSERT INTO regions (id, name) VALUES (%(id__0)s, %(name__0)s), (%(id__1)s, %(name__1)s), (%(id__2)s, %(name__2)s), (%(id__3)s, %(name__3)s), (%(id__4)s, %(name__4)s)\n",
      "2025-09-19 21:24:47,109 INFO sqlalchemy.engine.Engine [cached since 222s ago (insertmanyvalues) 1/1 (unordered)] {'name__0': 'Norte', 'id__0': '1', 'name__1': 'Nordeste', 'id__1': '2', 'name__2': 'Sudeste', 'id__2': '3', 'name__3': 'Sul', 'id__3': '4', 'name__4': 'Centro-Oeste', 'id__4': '5'}\n",
      "2025-09-19 21:24:47,116 INFO sqlalchemy.engine.Engine INSERT INTO timezones (name, utc_offset) VALUES (%(name__0)s, %(utc_offset__0)s), (%(name__1)s, %(utc_offset__1)s), (%(name__2)s, %(utc_offset__2)s), (%(name__3)s, %(utc_offset__3)s), (%(name__4)s, %(utc_offset__4)s), (%(name__5)s, %(utc_offset__5)s), (%(name__6)s, %(utc_offset__6)s)\n",
      "2025-09-19 21:24:47,117 INFO sqlalchemy.engine.Engine [cached since 222s ago (insertmanyvalues) 1/1 (unordered)] {'utc_offset__0': -2, 'name__0': 'America/Noronha', 'utc_offset__1': -3, 'name__1': 'America/Sao_Paulo', 'utc_offset__2': -3, 'name__2': 'America/Brasilia', 'utc_offset__3': -3, 'name__3': 'America/Recife', 'utc_offset__4': -4, 'name__4': 'America/Porto_Velho', 'utc_offset__5': -4, 'name__5': 'America/Manaus', 'utc_offset__6': -5, 'name__6': 'America/Rio_Branco'}\n",
      "2025-09-19 21:24:47,119 INFO sqlalchemy.engine.Engine COMMIT\n"
     ]
    }
   ],
   "source": [
    "# 1. Region Data\n",
    "regions_data = [\"Norte\", \"Nordeste\", \"Sudeste\", \"Sul\", \"Centro-Oeste\"]\n",
    "\n",
    "# 2. Timezone Data\n",
    "timezones_data = [\n",
    "    ('America/Noronha', -2),\n",
    "    ('America/Sao_Paulo', -3),\n",
    "    ('America/Brasilia', -3),\n",
    "    ('America/Recife', -3),\n",
    "    ('America/Porto_Velho', -4),\n",
    "    ('America/Manaus', -4),\n",
    "    ('America/Rio_Branco', -5),\n",
    "]\n",
    "\n",
    "Session = sessionmaker(bind=engine)\n",
    "\n",
    "with Session() as session:\n",
    "    # Populate the Regions table\n",
    "    regions = [\n",
    "        Region(id=str(idx), name=name) for idx, name in enumerate(regions_data, start=1)\n",
    "    ]\n",
    "    session.add_all(regions)\n",
    "\n",
    "    # Populate the Timezones table\n",
    "    timezones = [Timezone(name=name, utc_offset=offset) for name, offset in timezones_data]\n",
    "    session.add_all(timezones)\n",
    "\n",
    "    # Commit the changes to the database\n",
    "    session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4996ad7-1a1e-450d-8f42-f9fa0b1d128c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
