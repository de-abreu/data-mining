{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb25ad57-ce0a-4d7a-a867-a6ae947f0be8",
   "metadata": {},
   "source": [
    "## Autores\n",
    "\n",
    "| Nome | nUSP |\n",
    "| :--- | :--- |\n",
    "| Guilherme de Abreu Barreto | 12543033 |\n",
    "| Lucas Eduardo Gulka Pulcinelli | 12547336 |\n",
    "| Vinicio Yusuke Hayashibara | 13642797 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "804ab7b6-3450-43c2-afab-c3b6ace19568",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_DATABASE = \"postgres\"\n",
    "CENSO_DATABASE = \"censo2022\"\n",
    "USER = \"postgres\"\n",
    "PASSWORD = \"postgres\"\n",
    "HOST = \"localhost\"\n",
    "PORT = 5432\n",
    "URI = f\"postgresql+psycopg2://{USER}:{PASSWORD}@{HOST}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36e35410-453c-4bf3-9434-f6eaf4e2ab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from enum import Enum\n",
    "from math import sqrt\n",
    "from sqlalchemy import (\n",
    "    BigInteger,\n",
    "    Float,\n",
    "    Integer,\n",
    "    Index,\n",
    "    String,\n",
    "    CheckConstraint as constraint,\n",
    "    UniqueConstraint as unique,\n",
    "    PrimaryKeyConstraint as pkc,\n",
    "    ForeignKeyConstraint as fkc,\n",
    "    ForeignKey as fk,\n",
    "    JSON,\n",
    "    cast,\n",
    "    create_engine,\n",
    "    insert,\n",
    "    text,\n",
    "    func,\n",
    ")\n",
    "from sqlalchemy.orm import (\n",
    "    Mapped,\n",
    "    Session,\n",
    "    composite,\n",
    "    declarative_base,\n",
    "    relationship,\n",
    "    sessionmaker,\n",
    "    mapped_column as column,\n",
    ")\n",
    "from sqlalchemy.dialects.postgresql import ENUM\n",
    "from sqlalchemy.ext.hybrid import hybrid_method, hybrid_property\n",
    "from sqlalchemy.sql.schema import CheckConstraint\n",
    "from typing import Optional, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9363de9-0448-4cef-87cc-84107c3cc933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backref(back_populates: str) -> Mapped[Any]:\n",
    "    return relationship(back_populates=back_populates)\n",
    "\n",
    "\n",
    "def childOf(back_populates: str) -> Mapped[Any]:\n",
    "    return relationship(\n",
    "        back_populates=back_populates,\n",
    "        cascade=\"all, delete-orphan\",\n",
    "    )\n",
    "\n",
    "def digits(name:str) -> CheckConstraint:\n",
    "    return constraint(\"id ~ '^[0-9]+$'\", name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28f60461-1951-4e40-a91b-a0665447ae9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Households:\n",
    "    def __init__(self, urban: int, rural: int) -> None:\n",
    "        self.urban = urban\n",
    "        self.rural = rural\n",
    "\n",
    "    def __composite_values__(self) -> tuple[int, ...]:\n",
    "        return (self.urban, self.rural)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, Households) and \\\n",
    "               other.urban == self.urban and \\\n",
    "               other.rural == self.rural\n",
    "        \n",
    "    @hybrid_property\n",
    "    def total(self):\n",
    "        \"\"\"Python-side property for total households.\"\"\"\n",
    "        return self.urban + self.rural\n",
    "\n",
    "    @total.expression\n",
    "    def total(cls):\n",
    "        \"\"\"SQL-side expression for querying total households.\"\"\"\n",
    "        return cls.urban + cls.rural\n",
    "\n",
    "class Coordinate:\n",
    "    \"\"\"\n",
    "    A geographic coordinate point with longitude (lon) and latitude (lat) components.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, longitude: float, latitude: float) -> None:\n",
    "        self.longitude = longitude\n",
    "        self.latitude = latitude\n",
    "\n",
    "    @property\n",
    "    def longitude(self) -> float:\n",
    "        return self._longitude\n",
    "\n",
    "    @longitude.setter\n",
    "    def longitude(self, value: float) -> None:\n",
    "        if not (-180 <= value <= 180):\n",
    "            raise ValueError(f\"Longitude must be between -180 and 180 degrees, got {value}\")\n",
    "        self._longitude = value\n",
    "\n",
    "    @property\n",
    "    def latitude(self) -> float:\n",
    "        return self._latitude\n",
    "\n",
    "    @latitude.setter\n",
    "    def latitude(self, value: float) -> None:\n",
    "        if not (-90 <= value <= 90):\n",
    "            raise ValueError(f\"Latitude must be between -90 and 90 degrees, got {value}\")\n",
    "        self._latitude = value\n",
    "\n",
    "    def __composite_values__(self) -> tuple[float, ...]:\n",
    "        return (self.longitude, self.latitude)\n",
    "\n",
    "    def __eq__(self, other: \"Coordinate\") -> bool:\n",
    "        return isinstance(other, Coordinate) and \\\n",
    "               other.longitude == self.longitude and \\\n",
    "               other.latitude == self.latitude\n",
    "        \n",
    "    def __ne__ (self, other: \"Coordinate\") -> bool:\n",
    "        return not self.__eq__(other)\n",
    "\n",
    "    @hybrid_method\n",
    "    def distance(\n",
    "        self,\n",
    "        other: \"Coordinate\",\n",
    "        metric: str = 'euclidean'\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Calculate distance to another coordinate.\n",
    "        \n",
    "        Args:\n",
    "            other: Coordinate instance\n",
    "            metric: 'euclidean' or 'manhattan'\n",
    "        \n",
    "        Returns:\n",
    "            Distance between coordinates\n",
    "        \"\"\"\n",
    "        x = self.longitude - other.longitude\n",
    "        y = self.latitude - other.latitude\n",
    "        match metric:\n",
    "            case 'euclidean':\n",
    "                return sqrt(x**2 + y**2)\n",
    "            case 'manhattan':\n",
    "                return abs(x) + abs(y)\n",
    "            case _:\n",
    "                raise ValueError(\"Metric must be 'euclidean' or 'manhattan'\")\n",
    "\n",
    "    @distance.expression\n",
    "    def distance(\n",
    "        cls,\n",
    "        other_lon: float,\n",
    "        other_lat: float,\n",
    "        metric: str = 'euclidean'\n",
    "    ):\n",
    "        match metric:\n",
    "            case 'euclidean':\n",
    "                return func.sqrt(\n",
    "                    (cls.x - other_x) * (cls.x - other_x) +\n",
    "                    (cls.y - other_y) * (cls.y - other_y)\n",
    "                )\n",
    "            case 'manhattan':\n",
    "                return func.abs(cls.x - other_x) + func.abs(cls.y - other_y)\n",
    "            case _:\n",
    "                raise ValueError(\"Metric must be 'euclidean' or 'manhattan'\")\n",
    "\n",
    "\n",
    "class Biomes:\n",
    "    default: dict[str, float] = {\n",
    "        biome: 0.0 for biome in [\n",
    "            'amazon_rainforest',\n",
    "            'atlantic_forest',\n",
    "            'caatinga',\n",
    "            'cerrado',\n",
    "            'pantanal',\n",
    "            'pampas'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        self.distribution = kwargs\n",
    "\n",
    "    def __composite_values__(self) -> tuple[float, ...]:\n",
    "        return tuple(getattr(self, biome) for biome in self.default.keys())\n",
    "\n",
    "    @property\n",
    "    def distribution(self) -> dict[str, float]:\n",
    "        return {biome: getattr(self, biome) for biome in self.default.keys()}\n",
    "\n",
    "\n",
    "    @distribution.setter\n",
    "    def distribution(self, values: dict[str, float]) -> None:\n",
    "        merged_values = {**self.default, **values}\n",
    "\n",
    "        # Validation\n",
    "        invalid_keys = set(merged_values.keys()) - set(self.default.keys())\n",
    "        if invalid_keys:\n",
    "            raise ValueError(f\"Invalid biome types: {invalid_keys}. Valid types are: {list(self.default.keys())}\")\n",
    "        total = sum(merged_values.values())\n",
    "        if not (99.9 <= total <= 100.1):\n",
    "            raise ValueError(f\"Invalid biome distribution, totalling {total:.1f}%\")\n",
    "        self._distribution = merged_values\n",
    "\n",
    "        for biome_type, value in merged_values.items():\n",
    "            setattr(self, biome_type, value)\n",
    "\n",
    "    @property\n",
    "    def total(self) -> float:\n",
    "        return sum(getattr(self, biome) for biome in self.default.keys())\n",
    "\n",
    "    @classmethod\n",
    "    def toList(cls) -> list[str]:\n",
    "        return list(cls.default.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a55a5be-368e-4285-b77b-da320d68b487",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_enum = ENUM(*[\n",
    "    \"AC\", \"AL\", \"AP\", \"AM\", \"BA\", \"CE\", \"DF\", \"ES\", \"GO\", \"MA\", \n",
    "    \"MT\", \"MS\", \"MG\", \"PA\", \"PB\", \"PR\", \"PE\", \"PI\", \"RJ\", \"RN\", \n",
    "    \"RS\", \"RO\", \"RR\", \"SC\", \"SP\", \"SE\", \"TO\"\n",
    "], name=\"state_enum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "198f174f-0553-4092-8f21-9d1f041c0ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "class Region(Base):\n",
    "    __tablename__: str = \"regions\"\n",
    "\n",
    "    # Attributes\n",
    "    id: Mapped[str] = column(\n",
    "        String(1),\n",
    "        digits(\"ck_region_id\"),\n",
    "        primary_key=True\n",
    "    )\n",
    "    name: Mapped[str] = column(unique=True)\n",
    "\n",
    "    # Relationships\n",
    "    states: Mapped[list[\"State\"]] = childOf('region')\n",
    "\n",
    "\n",
    "class State(Base):\n",
    "    __tablename__: str = \"states\"\n",
    "\n",
    "    # Attributes\n",
    "    id: Mapped[str] = column(String(1), digits(\"ck_state_id\"))\n",
    "    name: Mapped[str] = column(unique=True)\n",
    "    uf: Mapped[str] = column(state_enum, unique=True)\n",
    "    location: Mapped[Coordinate] = composite(\n",
    "        column(\"longitude\", Float),\n",
    "        column(\"latitude\", Float)\n",
    "    )\n",
    "    area: Mapped[float]\n",
    "    biome_distribution: Mapped[Biomes] = composite(\n",
    "        *[column(biome, Float, default=0.0) for biome in Biomes.toList()]\n",
    "    )\n",
    "\n",
    "    # Foreign keys\n",
    "    region_id: Mapped[str] = column(fk(\"regions.id\"))\n",
    "\n",
    "    # Relationships\n",
    "    region: Mapped[\"Region\"] = backref(\"states\")\n",
    "    cities: Mapped[list[\"City\"]] = childOf(\"state\")\n",
    "\n",
    "    __table_args__: tuple[pkc, unique,] = (\n",
    "        pkc(\"region_id\", \"id\"),\n",
    "        unique('longitude', 'latitude', name='uq_state_location'),\n",
    "    )\n",
    "\n",
    "\n",
    "class City(Base):\n",
    "    __tablename__: str = \"cities\"\n",
    "\n",
    "    # Attributes\n",
    "    id: Mapped[str] = column(String(5), digits(\"ck_city_id\"))\n",
    "    name: Mapped[str]\n",
    "    is_capital: Mapped[bool] = column(default=False, server_default='false')\n",
    "    location: Mapped[Coordinate] = composite(\n",
    "        column(\"longitude\", Float),\n",
    "        column(\"latitude\", Float)\n",
    "    )\n",
    "    ddd: Mapped[str] = column(String(2), digits(\"ck_city_ddd\"))\n",
    "    households: Mapped[Households] = composite(\n",
    "        column(\"urban\", Integer),\n",
    "        column(\"rural\", Integer)\n",
    "    )\n",
    "    population_race: Mapped[dict] = column(JSON)\n",
    "    population_education: Mapped[dict] = column(JSON)\n",
    "    \n",
    "\n",
    "    # Foreign keys\n",
    "    timezone_name: Mapped[str] = column(fk(\"timezones.name\"))\n",
    "    region_id: Mapped[str]\n",
    "    state_id: Mapped[str]\n",
    "\n",
    "    # Relationships\n",
    "    timezone: Mapped[\"Timezone\"] = backref(\"cities\")\n",
    "    state: Mapped[\"State\"] = backref(\"cities\")\n",
    "\n",
    "    @hybrid_property\n",
    "    def ibge_code(self) -> str:\n",
    "        \"\"\"Python-side property to get the IBGE code.\"\"\"\n",
    "        return self.region_id + self.state_id + self.id\n",
    "\n",
    "    @ibge_code.expression\n",
    "    def ibge_code(cls):\n",
    "        \"\"\"SQL-side expression for querying.\"\"\"\n",
    "        return cast(\n",
    "            func.concat(\n",
    "                # Join to State to get the region ID\n",
    "                cast(cls.region_id, String),\n",
    "                cast(cls.state_id, String),\n",
    "                cast(cls.id, String)\n",
    "            ),\n",
    "            BigInteger\n",
    "        )\n",
    "\n",
    "    __table_args__: tuple[pkc, fkc, unique, ] = (\n",
    "        pkc(\"region_id\", \"state_id\", \"id\"),\n",
    "        fkc(\n",
    "            ['region_id', 'state_id'],\n",
    "            ['states.region_id', 'states.id'],\n",
    "            name='fk_region_composite'\n",
    "        ),\n",
    "        unique(\"longitude\", \"latitude\", name=\"uq_city_location\"),\n",
    "        Index(\n",
    "            'state_capitals_index'\n",
    "            'region_id', 'state_id',\n",
    "            postgresql_where=text('is_capital'),\n",
    "            unique=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "class Timezone(Base):\n",
    "    __tablename__: str = \"timezones\"\n",
    "\n",
    "    # Attributes\n",
    "    name: Mapped[str] = column(primary_key=True)\n",
    "    utc_offset: Mapped[int]\n",
    "\n",
    "    # Relationships\n",
    "    cities: Mapped[list[\"City\"]] = backref('timezone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b7d8296-2d1b-4358-92f0-b1a304f6875d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-19 23:17:57,434 INFO sqlalchemy.engine.Engine select pg_catalog.version()\n",
      "2025-09-19 23:17:57,435 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2025-09-19 23:17:57,437 INFO sqlalchemy.engine.Engine select current_schema()\n",
      "2025-09-19 23:17:57,438 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2025-09-19 23:17:57,441 INFO sqlalchemy.engine.Engine show standard_conforming_strings\n",
      "2025-09-19 23:17:57,442 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2025-09-19 23:17:57,447 INFO sqlalchemy.engine.Engine BEGIN (implicit; DBAPI should not BEGIN due to autocommit mode)\n",
      "2025-09-19 23:17:57,448 INFO sqlalchemy.engine.Engine CREATE DATABASE censo2022\n",
      "2025-09-19 23:17:57,449 INFO sqlalchemy.engine.Engine [generated in 0.00189s] {}\n",
      "2025-09-19 23:17:57,501 INFO sqlalchemy.engine.Engine ROLLBACK using DBAPI connection.rollback(), DBAPI should ignore due to autocommit mode\n"
     ]
    }
   ],
   "source": [
    "engine = create_engine(URI + DEFAULT_DATABASE, echo=True)\n",
    "\n",
    "with engine.connect().execution_options(isolation_level=\"AUTOCOMMIT\") as conn:\n",
    "    conn.execute(text(f\"CREATE DATABASE {CENSO_DATABASE}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07a4df84-58fa-458a-a1a3-e5707bad84c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-19 23:17:57,520 INFO sqlalchemy.engine.Engine select pg_catalog.version()\n",
      "2025-09-19 23:17:57,521 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2025-09-19 23:17:57,524 INFO sqlalchemy.engine.Engine select current_schema()\n",
      "2025-09-19 23:17:57,525 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2025-09-19 23:17:57,528 INFO sqlalchemy.engine.Engine show standard_conforming_strings\n",
      "2025-09-19 23:17:57,530 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2025-09-19 23:17:57,531 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-09-19 23:17:57,538 INFO sqlalchemy.engine.Engine SELECT pg_catalog.pg_class.relname \n",
      "FROM pg_catalog.pg_class JOIN pg_catalog.pg_namespace ON pg_catalog.pg_namespace.oid = pg_catalog.pg_class.relnamespace \n",
      "WHERE pg_catalog.pg_class.relname = %(table_name)s AND pg_catalog.pg_class.relkind = ANY (ARRAY[%(param_1)s, %(param_2)s, %(param_3)s, %(param_4)s, %(param_5)s]) AND pg_catalog.pg_table_is_visible(pg_catalog.pg_class.oid) AND pg_catalog.pg_namespace.nspname != %(nspname_1)s\n",
      "2025-09-19 23:17:57,538 INFO sqlalchemy.engine.Engine [generated in 0.00080s] {'table_name': 'regions', 'param_1': 'r', 'param_2': 'p', 'param_3': 'f', 'param_4': 'v', 'param_5': 'm', 'nspname_1': 'pg_catalog'}\n",
      "2025-09-19 23:17:57,542 INFO sqlalchemy.engine.Engine SELECT pg_catalog.pg_class.relname \n",
      "FROM pg_catalog.pg_class JOIN pg_catalog.pg_namespace ON pg_catalog.pg_namespace.oid = pg_catalog.pg_class.relnamespace \n",
      "WHERE pg_catalog.pg_class.relname = %(table_name)s AND pg_catalog.pg_class.relkind = ANY (ARRAY[%(param_1)s, %(param_2)s, %(param_3)s, %(param_4)s, %(param_5)s]) AND pg_catalog.pg_table_is_visible(pg_catalog.pg_class.oid) AND pg_catalog.pg_namespace.nspname != %(nspname_1)s\n",
      "2025-09-19 23:17:57,544 INFO sqlalchemy.engine.Engine [cached since 0.006855s ago] {'table_name': 'states', 'param_1': 'r', 'param_2': 'p', 'param_3': 'f', 'param_4': 'v', 'param_5': 'm', 'nspname_1': 'pg_catalog'}\n",
      "2025-09-19 23:17:57,547 INFO sqlalchemy.engine.Engine SELECT pg_catalog.pg_class.relname \n",
      "FROM pg_catalog.pg_class JOIN pg_catalog.pg_namespace ON pg_catalog.pg_namespace.oid = pg_catalog.pg_class.relnamespace \n",
      "WHERE pg_catalog.pg_class.relname = %(table_name)s AND pg_catalog.pg_class.relkind = ANY (ARRAY[%(param_1)s, %(param_2)s, %(param_3)s, %(param_4)s, %(param_5)s]) AND pg_catalog.pg_table_is_visible(pg_catalog.pg_class.oid) AND pg_catalog.pg_namespace.nspname != %(nspname_1)s\n",
      "2025-09-19 23:17:57,548 INFO sqlalchemy.engine.Engine [cached since 0.0101s ago] {'table_name': 'cities', 'param_1': 'r', 'param_2': 'p', 'param_3': 'f', 'param_4': 'v', 'param_5': 'm', 'nspname_1': 'pg_catalog'}\n",
      "2025-09-19 23:17:57,550 INFO sqlalchemy.engine.Engine SELECT pg_catalog.pg_class.relname \n",
      "FROM pg_catalog.pg_class JOIN pg_catalog.pg_namespace ON pg_catalog.pg_namespace.oid = pg_catalog.pg_class.relnamespace \n",
      "WHERE pg_catalog.pg_class.relname = %(table_name)s AND pg_catalog.pg_class.relkind = ANY (ARRAY[%(param_1)s, %(param_2)s, %(param_3)s, %(param_4)s, %(param_5)s]) AND pg_catalog.pg_table_is_visible(pg_catalog.pg_class.oid) AND pg_catalog.pg_namespace.nspname != %(nspname_1)s\n",
      "2025-09-19 23:17:57,551 INFO sqlalchemy.engine.Engine [cached since 0.01326s ago] {'table_name': 'timezones', 'param_1': 'r', 'param_2': 'p', 'param_3': 'f', 'param_4': 'v', 'param_5': 'm', 'nspname_1': 'pg_catalog'}\n",
      "2025-09-19 23:17:57,554 INFO sqlalchemy.engine.Engine SELECT pg_catalog.pg_type.typname \n",
      "FROM pg_catalog.pg_type JOIN pg_catalog.pg_namespace ON pg_catalog.pg_namespace.oid = pg_catalog.pg_type.typnamespace \n",
      "WHERE pg_catalog.pg_type.typname = %(typname_1)s AND pg_catalog.pg_type_is_visible(pg_catalog.pg_type.oid) AND pg_catalog.pg_namespace.nspname != %(nspname_1)s\n",
      "2025-09-19 23:17:57,555 INFO sqlalchemy.engine.Engine [generated in 0.00080s] {'typname_1': 'state_enum', 'nspname_1': 'pg_catalog'}\n",
      "2025-09-19 23:17:57,559 INFO sqlalchemy.engine.Engine CREATE TYPE state_enum AS ENUM ('AC', 'AL', 'AP', 'AM', 'BA', 'CE', 'DF', 'ES', 'GO', 'MA', 'MT', 'MS', 'MG', 'PA', 'PB', 'PR', 'PE', 'PI', 'RJ', 'RN', 'RS', 'RO', 'RR', 'SC', 'SP', 'SE', 'TO')\n",
      "2025-09-19 23:17:57,560 INFO sqlalchemy.engine.Engine [no key 0.00095s] {}\n",
      "2025-09-19 23:17:57,567 INFO sqlalchemy.engine.Engine \n",
      "CREATE TABLE regions (\n",
      "\tid VARCHAR(1) NOT NULL CONSTRAINT ck_region_id CHECK (id ~ '^[0-9]+$'), \n",
      "\tname VARCHAR NOT NULL, \n",
      "\tPRIMARY KEY (id), \n",
      "\tUNIQUE (name)\n",
      ")\n",
      "\n",
      "\n",
      "2025-09-19 23:17:57,568 INFO sqlalchemy.engine.Engine [no key 0.00085s] {}\n",
      "2025-09-19 23:17:57,583 INFO sqlalchemy.engine.Engine \n",
      "CREATE TABLE timezones (\n",
      "\tname VARCHAR NOT NULL, \n",
      "\tutc_offset INTEGER NOT NULL, \n",
      "\tPRIMARY KEY (name)\n",
      ")\n",
      "\n",
      "\n",
      "2025-09-19 23:17:57,584 INFO sqlalchemy.engine.Engine [no key 0.00095s] {}\n",
      "2025-09-19 23:17:57,597 INFO sqlalchemy.engine.Engine \n",
      "CREATE TABLE states (\n",
      "\tid VARCHAR(1) NOT NULL CONSTRAINT ck_state_id CHECK (id ~ '^[0-9]+$'), \n",
      "\tname VARCHAR NOT NULL, \n",
      "\tuf state_enum NOT NULL, \n",
      "\tlongitude FLOAT, \n",
      "\tlatitude FLOAT, \n",
      "\tarea FLOAT NOT NULL, \n",
      "\tamazon_rainforest FLOAT, \n",
      "\tatlantic_forest FLOAT, \n",
      "\tcaatinga FLOAT, \n",
      "\tcerrado FLOAT, \n",
      "\tpantanal FLOAT, \n",
      "\tpampas FLOAT, \n",
      "\tregion_id VARCHAR(1) NOT NULL, \n",
      "\tPRIMARY KEY (region_id, id), \n",
      "\tCONSTRAINT uq_state_location UNIQUE (longitude, latitude), \n",
      "\tUNIQUE (name), \n",
      "\tUNIQUE (uf), \n",
      "\tFOREIGN KEY(region_id) REFERENCES regions (id)\n",
      ")\n",
      "\n",
      "\n",
      "2025-09-19 23:17:57,598 INFO sqlalchemy.engine.Engine [no key 0.00074s] {}\n",
      "2025-09-19 23:17:57,622 INFO sqlalchemy.engine.Engine \n",
      "CREATE TABLE cities (\n",
      "\tid VARCHAR(5) NOT NULL CONSTRAINT ck_city_id CHECK (id ~ '^[0-9]+$'), \n",
      "\tname VARCHAR NOT NULL, \n",
      "\tis_capital BOOLEAN DEFAULT 'false' NOT NULL, \n",
      "\tlongitude FLOAT, \n",
      "\tlatitude FLOAT, \n",
      "\tddd VARCHAR(2) NOT NULL CONSTRAINT ck_city_ddd CHECK (id ~ '^[0-9]+$'), \n",
      "\turban INTEGER, \n",
      "\trural INTEGER, \n",
      "\tpopulation_race JSON NOT NULL, \n",
      "\tpopulation_education JSON NOT NULL, \n",
      "\ttimezone_name VARCHAR NOT NULL, \n",
      "\tregion_id VARCHAR NOT NULL, \n",
      "\tstate_id VARCHAR NOT NULL, \n",
      "\tPRIMARY KEY (region_id, state_id, id), \n",
      "\tCONSTRAINT fk_region_composite FOREIGN KEY(region_id, state_id) REFERENCES states (region_id, id), \n",
      "\tCONSTRAINT uq_city_location UNIQUE (longitude, latitude), \n",
      "\tFOREIGN KEY(timezone_name) REFERENCES timezones (name)\n",
      ")\n",
      "\n",
      "\n",
      "2025-09-19 23:17:57,623 INFO sqlalchemy.engine.Engine [no key 0.00100s] {}\n",
      "2025-09-19 23:17:57,638 INFO sqlalchemy.engine.Engine CREATE UNIQUE INDEX state_capitals_indexregion_id ON cities (state_id) WHERE is_capital\n",
      "2025-09-19 23:17:57,639 INFO sqlalchemy.engine.Engine [no key 0.00070s] {}\n",
      "2025-09-19 23:17:57,647 INFO sqlalchemy.engine.Engine COMMIT\n"
     ]
    }
   ],
   "source": [
    "engine = create_engine(URI + CENSO_DATABASE, echo=True)\n",
    "Session = sessionmaker(bind=engine)\n",
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52cf92cf-898b-4989-a0a7-438390d5c77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-19 23:17:57,683 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-09-19 23:17:57,685 INFO sqlalchemy.engine.Engine INSERT INTO regions (id, name) VALUES (%(id__0)s, %(name__0)s), (%(id__1)s, %(name__1)s), (%(id__2)s, %(name__2)s), (%(id__3)s, %(name__3)s), (%(id__4)s, %(name__4)s)\n",
      "2025-09-19 23:17:57,686 INFO sqlalchemy.engine.Engine [generated in 0.00008s (insertmanyvalues) 1/1 (unordered)] {'name__0': 'Norte', 'id__0': '1', 'name__1': 'Nordeste', 'id__1': '2', 'name__2': 'Sudeste', 'id__2': '3', 'name__3': 'Sul', 'id__3': '4', 'name__4': 'Centro-Oeste', 'id__4': '5'}\n",
      "2025-09-19 23:17:57,690 INFO sqlalchemy.engine.Engine INSERT INTO timezones (name, utc_offset) VALUES (%(name__0)s, %(utc_offset__0)s), (%(name__1)s, %(utc_offset__1)s), (%(name__2)s, %(utc_offset__2)s), (%(name__3)s, %(utc_offset__3)s), (%(name__4)s, %(utc_offset__4)s), (%(name__5)s, %(utc_offset__5)s), (%(name__6)s, %(utc_offset__6)s)\n",
      "2025-09-19 23:17:57,692 INFO sqlalchemy.engine.Engine [generated in 0.00041s (insertmanyvalues) 1/1 (unordered)] {'name__0': 'America/Noronha', 'utc_offset__0': -2, 'name__1': 'America/Sao_Paulo', 'utc_offset__1': -3, 'name__2': 'America/Brasilia', 'utc_offset__2': -3, 'name__3': 'America/Recife', 'utc_offset__3': -3, 'name__4': 'America/Porto_Velho', 'utc_offset__4': -4, 'name__5': 'America/Manaus', 'utc_offset__5': -4, 'name__6': 'America/Rio_Branco', 'utc_offset__6': -5}\n",
      "2025-09-19 23:17:57,697 INFO sqlalchemy.engine.Engine COMMIT\n"
     ]
    }
   ],
   "source": [
    "# 1. Region Data\n",
    "regions_data = [\"Norte\", \"Nordeste\", \"Sudeste\", \"Sul\", \"Centro-Oeste\"]\n",
    "\n",
    "# 2. Timezone Data\n",
    "timezones_data = [\n",
    "    ('America/Noronha', -2),\n",
    "    ('America/Sao_Paulo', -3),\n",
    "    ('America/Brasilia', -3),\n",
    "    ('America/Recife', -3),\n",
    "    ('America/Porto_Velho', -4),\n",
    "    ('America/Manaus', -4),\n",
    "    ('America/Rio_Branco', -5),\n",
    "]\n",
    "\n",
    "with Session() as session:\n",
    "    # Populate the Regions table\n",
    "    regions = [\n",
    "        Region(id=str(idx), name=name) for idx, name in enumerate(regions_data, start=1)\n",
    "    ]\n",
    "    session.add_all(regions)\n",
    "\n",
    "    # Populate the Timezones table\n",
    "    timezones = [Timezone(name=name, utc_offset=offset) for name, offset in timezones_data]\n",
    "    session.add_all(timezones)\n",
    "\n",
    "    # Commit the changes to the database\n",
    "    session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4996ad7-1a1e-450d-8f42-f9fa0b1d128c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-19 23:17:57,720 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-09-19 23:17:57,726 INFO sqlalchemy.engine.Engine INSERT INTO states (id, name, uf, longitude, latitude, area, amazon_rainforest, atlantic_forest, caatinga, cerrado, pantanal, pampas, region_id) VALUES (%(id__0)s, %(name__0)s, %(uf__0)s, %(longitude__0)s, %(latitude__0)s, %(area__0)s, %(amazon_rainf ... 5907 characters truncated ... orest__26)s, %(caatinga__26)s, %(cerrado__26)s, %(pantanal__26)s, %(pampas__26)s, %(region_id__26)s)\n",
      "2025-09-19 23:17:57,727 INFO sqlalchemy.engine.Engine [generated in 0.00073s (insertmanyvalues) 1/1 (unordered)] {'id__0': '1', 'longitude__0': -63.34, 'pampas__0': 0.0, 'latitude__0': -10.83, 'caatinga__0': 0.0, 'atlantic_forest__0': 0.0, 'uf__0': 'RO', 'region_id__0': '1', 'cerrado__0': 1.0, 'pantanal__0': 0.0, 'amazon_rainforest__0': 99.0, 'name__0': 'Rondônia', 'area__0': 237754.171, 'id__1': '2', 'longitude__1': -70.55, 'pampas__1': 0.0, 'latitude__1': -8.77, 'caatinga__1': 0.0, 'atlantic_forest__1': 0.0, 'uf__1': 'AC', 'region_id__1': '1', 'cerrado__1': 0.0, 'pantanal__1': 0.0, 'amazon_rainforest__1': 100.0, 'name__1': 'Acre', 'area__1': 164082.96, 'id__2': '3', 'longitude__2': -65.1, 'pampas__2': 0.0, 'latitude__2': -3.47, 'caatinga__2': 0.0, 'atlantic_forest__2': 0.0, 'uf__2': 'AM', 'region_id__2': '1', 'cerrado__2': 0.0, 'pantanal__2': 0.0, 'amazon_rainforest__2': 100.0, 'name__2': 'Amazonas', 'area__2': 1558706.127, 'id__3': '4', 'longitude__3': -61.33, 'pampas__3': 0.0, 'latitude__3': 1.99, 'caatinga__3': 0.0, 'atlantic_forest__3': 0.0, 'uf__3': 'RR', 'region_id__3': '1', 'cerrado__3': 0.0, 'pantanal__3': 0.0, 'amazon_rainforest__3': 100.0 ... 251 parameters truncated ... 'pampas__23': 0.0, 'latitude__23': -20.51, 'caatinga__23': 0.0, 'atlantic_forest__23': 14.0, 'uf__23': 'MS', 'region_id__23': '5', 'cerrado__23': 61.0, 'pantanal__23': 25.0, 'amazon_rainforest__23': 0.0, 'name__23': 'Mato Grosso do Sul', 'area__23': 357142.01, 'id__24': '1', 'longitude__24': -55.42, 'pampas__24': 0.0, 'latitude__24': -12.64, 'caatinga__24': 0.0, 'atlantic_forest__24': 0.0, 'uf__24': 'MT', 'region_id__24': '5', 'cerrado__24': 39.0, 'pantanal__24': 7.0, 'amazon_rainforest__24': 54.0, 'name__24': 'Mato Grosso', 'area__24': 903208.362, 'id__25': '2', 'longitude__25': -49.86, 'pampas__25': 0.0, 'latitude__25': -15.98, 'caatinga__25': 0.0, 'atlantic_forest__25': 3.0, 'uf__25': 'GO', 'region_id__25': '5', 'cerrado__25': 97.0, 'pantanal__25': 0.0, 'amazon_rainforest__25': 0.0, 'name__25': 'Goias', 'area__25': 340242.86, 'id__26': '3', 'longitude__26': -47.86, 'pampas__26': 0.0, 'latitude__26': -15.83, 'caatinga__26': 0.0, 'atlantic_forest__26': 0.0, 'uf__26': 'DF', 'region_id__26': '5', 'cerrado__26': 100.0, 'pantanal__26': 0.0, 'amazon_rainforest__26': 0.0, 'name__26': 'Distrito Federal', 'area__26': 5760.783}\n",
      "2025-09-19 23:17:57,738 INFO sqlalchemy.engine.Engine COMMIT\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('datasets/BREstados.csv')\n",
    "df.fillna(0.0, inplace=True)\n",
    "BIOME_COLUMN_MAPPING = {\n",
    "    'Amazônia': 'amazon_rainforest',\n",
    "    'Mata Atlântica': 'atlantic_forest',\n",
    "    'Caatinga': 'caatinga',\n",
    "    'Cerrado': 'cerrado',\n",
    "    'Pantanal': 'pantanal',\n",
    "    'Pampa': 'pampas'\n",
    "}\n",
    "csv_biome_cols = df.columns[8:].tolist()\n",
    "\n",
    "states_to_insert = []\n",
    "for _, row in df.iterrows():\n",
    "    code_str = str(row['codigouf'])\n",
    "    state_data = {\n",
    "        'region_id': code_str[0],\n",
    "        'id': code_str[1],\n",
    "        'uf': row['uf'],\n",
    "        'name': row['estado'],\n",
    "        'longitude': row['long'],\n",
    "        'latitude': row['lat'],\n",
    "        'area': row['Area']\n",
    "    }\n",
    "\n",
    "    for csv_col in csv_biome_cols:\n",
    "        model_attr = BIOME_COLUMN_MAPPING.get(csv_col)\n",
    "        state_data[model_attr] = row[csv_col]\n",
    "\n",
    "    states_to_insert.append(state_data)\n",
    "\n",
    "\n",
    "with Session() as session:\n",
    "    session.bulk_insert_mappings(State, states_to_insert)\n",
    "    session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb863b8f-78a4-4adc-a3cb-61ab26decd73",
   "metadata": {},
   "source": [
    "Ok, now to the city table, this one will be more complicated as we'll need to fetch its data from multiple csv sources. Here is the City class:\n",
    "\n",
    "```python\n",
    "\n",
    "class City(Base):\n",
    "\n",
    "    __tablename__: str = \"cities\"\n",
    "\n",
    "\n",
    "    # Attributes\n",
    "\n",
    "    id: Mapped[str] = column(String(5), digits(\"ck_city_id\"))\n",
    "\n",
    "    name: Mapped[str]\n",
    "\n",
    "    is_capital: Mapped[bool] = column(default=False, server_default='false')\n",
    "\n",
    "    location: Mapped[Coordinate] = composite(\n",
    "\n",
    "        column(\"longitude\", Float),\n",
    "\n",
    "        column(\"latitude\", Float)\n",
    "\n",
    "    )\n",
    "\n",
    "    ddd: Mapped[str] = column(String(2), digits(\"ck_city_ddd\"))\n",
    "\n",
    "    households: Mapped[Households] = composite(\n",
    "\n",
    "        column(\"urban\", Integer),\n",
    "\n",
    "        column(\"rural\", Integer)\n",
    "\n",
    "    )\n",
    "\n",
    "    population_race: Mapped[dict] = column(JSON)\n",
    "\n",
    "    population_education: Mapped[dict] = column(JSON)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # Foreign keys\n",
    "\n",
    "    timezone_name: Mapped[str] = column(fk(\"timezones.name\"))\n",
    "\n",
    "    region_id: Mapped[str]\n",
    "\n",
    "    state_id: Mapped[str]\n",
    "\n",
    "\n",
    "    # Relationships\n",
    "\n",
    "    timezone: Mapped[\"Timezone\"] = backref(\"cities\")\n",
    "\n",
    "    state: Mapped[\"State\"] = backref(\"cities\")\n",
    "\n",
    "\n",
    "    @hybrid_property\n",
    "\n",
    "    def ibge_code(self) -> str:\n",
    "\n",
    "        \"\"\"Python-side property to get the IBGE code.\"\"\"\n",
    "\n",
    "        return self.region_id + self.state_id + self.id\n",
    "\n",
    "\n",
    "    @ibge_code.expression\n",
    "\n",
    "    def ibge_code(cls):\n",
    "\n",
    "        \"\"\"SQL-side expression for querying.\"\"\"\n",
    "\n",
    "        return cast(\n",
    "\n",
    "            func.concat(\n",
    "\n",
    "                # Join to State to get the region ID\n",
    "\n",
    "                cast(cls.region_id, String),\n",
    "\n",
    "                cast(cls.state_id, String),\n",
    "\n",
    "                cast(cls.id, String)\n",
    "\n",
    "            ),\n",
    "\n",
    "            BigInteger\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "    __table_args__: tuple[pkc, fkc, unique, ] = (\n",
    "\n",
    "        pkc(\"region_id\", \"state_id\", \"id\"),\n",
    "\n",
    "        fkc(\n",
    "\n",
    "            ['region_id', 'state_id'],\n",
    "\n",
    "            ['states.region_id', 'states.id'],\n",
    "\n",
    "            name='fk_region_composite'\n",
    "\n",
    "        ),\n",
    "\n",
    "        unique(\"longitude\", \"latitude\", name=\"uq_city_location\"),\n",
    "\n",
    "        Index(\n",
    "\n",
    "            'state_capitals_index'\n",
    "\n",
    "            'region_id', 'state_id',\n",
    "\n",
    "            postgresql_where=text('is_capital'),\n",
    "\n",
    "            unique=True,\n",
    "\n",
    "        )\n",
    "\n",
    "    )\n",
    "\n",
    "```\n",
    "\n",
    "From the first csv file we should fetch the following values in the following columns:\n",
    "\n",
    "- codigo_ibge: region_id is the first digit of the code, state_id is the second, the rest is the id\n",
    "\n",
    "- nome: map to name\n",
    "\n",
    "- latitude, longitude and ddd: map to the columns of same name.\n",
    "\n",
    "- capital: map to is_capital\n",
    "\n",
    "- fuso_horario: map to timezone_name\n",
    "\n",
    "For the second csv file we have the following columns of interest:\n",
    "\n",
    "- Município: a city name followed by a state uf code in parenthesis. As uf codes are unique, this should identify a city of same name in that state\n",
    "\n",
    "- Urbana and Rural: should be mapped to the columns urban and rural respectively.\n",
    "\n",
    "For the third csv file "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
