{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb25ad57-ce0a-4d7a-a867-a6ae947f0be8",
   "metadata": {},
   "source": [
    "## Autores\n",
    "\n",
    "| Nome | nUSP |\n",
    "| :--- | :--- |\n",
    "| Guilherme de Abreu Barreto | 12543033 |\n",
    "| Lucas Eduardo Gulka Pulcinelli | 12547336 |\n",
    "| Vinicio Yusuke Hayashibara | 13642797 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "804ab7b6-3450-43c2-afab-c3b6ace19568",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_DATABASE = \"postgres\"\n",
    "CENSO_DATABASE = \"censo2022\"\n",
    "USER = \"postgres\"\n",
    "PASSWORD = \"postgres\"\n",
    "HOST = \"localhost\"\n",
    "PORT = 5432\n",
    "URI = f\"postgresql+psycopg2://{USER}:{PASSWORD}@{HOST}/\"\n",
    "FILES = {\n",
    "    'states': {\n",
    "        'filepath_or_buffer': 'datasets/BREstados.csv',\n",
    "    },\n",
    "    'cities': {\n",
    "        'filepath_or_buffer': 'datasets/TabelaMunicipios.csv',\n",
    "    },\n",
    "    'households': {\n",
    "        'filepath_or_buffer': 'datasets/tabela9923.csv',\n",
    "    },\n",
    "    'race': {\n",
    "        'filepath_or_buffer': 'datasets/tabela9606.csv',\n",
    "        'sep': ';',\n",
    "        'header': None,\n",
    "        'skiprows': 1,\n",
    "    },\n",
    "    'education': {\n",
    "        'filepath_or_buffer': 'datasets/tabela10065.csv',\n",
    "        'header': None,\n",
    "        'skiprows': 1,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36e35410-453c-4bf3-9434-f6eaf4e2ab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import unicodedata\n",
    "import re\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass, fields\n",
    "from math import sqrt\n",
    "from sqlalchemy import (\n",
    "    BigInteger,\n",
    "    Float,\n",
    "    Integer,\n",
    "    Index,\n",
    "    String,\n",
    "    CheckConstraint as constraint,\n",
    "    UniqueConstraint as unique,\n",
    "    PrimaryKeyConstraint as pkc,\n",
    "    ForeignKeyConstraint as fkc,\n",
    "    ForeignKey as fk,\n",
    "    JSON,\n",
    "    cast,\n",
    "    create_engine,\n",
    "    insert,\n",
    "    text,\n",
    "    func,\n",
    "    select,\n",
    ")\n",
    "from sqlalchemy.orm import (\n",
    "    Mapped,\n",
    "    Session,\n",
    "    composite,\n",
    "    declarative_base,\n",
    "    declared_attr,\n",
    "    relationship,\n",
    "    sessionmaker,\n",
    "    mapped_column as column,\n",
    "    validates\n",
    ")\n",
    "from sqlalchemy.ext.hybrid import hybrid_method, hybrid_property\n",
    "from sqlalchemy.sql.schema import CheckConstraint\n",
    "from typing import Optional, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6346f187-0d29-4c81-9bcd-a26920f601c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-20 17:39:09,609 INFO sqlalchemy.engine.Engine select pg_catalog.version()\n",
      "2025-09-20 17:39:09,610 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2025-09-20 17:39:09,614 INFO sqlalchemy.engine.Engine select current_schema()\n",
      "2025-09-20 17:39:09,614 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2025-09-20 17:39:09,616 INFO sqlalchemy.engine.Engine show standard_conforming_strings\n",
      "2025-09-20 17:39:09,617 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2025-09-20 17:39:09,620 INFO sqlalchemy.engine.Engine BEGIN (implicit; DBAPI should not BEGIN due to autocommit mode)\n",
      "2025-09-20 17:39:09,623 INFO sqlalchemy.engine.Engine \n",
      "        SELECT pg_terminate_backend(pid)\n",
      "        FROM pg_stat_activity\n",
      "        WHERE datname = 'censo2022';\n",
      "    \n",
      "2025-09-20 17:39:09,627 INFO sqlalchemy.engine.Engine [generated in 0.00678s] {}\n",
      "2025-09-20 17:39:09,632 INFO sqlalchemy.engine.Engine DROP DATABASE IF EXISTS censo2022;\n",
      "2025-09-20 17:39:09,633 INFO sqlalchemy.engine.Engine [generated in 0.00071s] {}\n",
      "2025-09-20 17:39:09,662 INFO sqlalchemy.engine.Engine CREATE DATABASE censo2022;\n",
      "2025-09-20 17:39:09,663 INFO sqlalchemy.engine.Engine [generated in 0.00093s] {}\n",
      "2025-09-20 17:39:09,714 INFO sqlalchemy.engine.Engine ROLLBACK using DBAPI connection.rollback(), DBAPI should ignore due to autocommit mode\n"
     ]
    }
   ],
   "source": [
    "engine = create_engine(URI + DEFAULT_DATABASE, echo=True)\n",
    "\n",
    "with engine.connect().execution_options(isolation_level=\"AUTOCOMMIT\") as conn:\n",
    "    terminate_sql = text(f\"\"\"\n",
    "        SELECT pg_terminate_backend(pid)\n",
    "        FROM pg_stat_activity\n",
    "        WHERE datname = '{CENSO_DATABASE}';\n",
    "    \"\"\")\n",
    "    try:\n",
    "        conn.execute(terminate_sql)\n",
    "    except ProgrammingError as e:\n",
    "        print(f\"Could not terminate connections (this is often normal): {e}\")\n",
    "    conn.execute(text(f\"DROP DATABASE IF EXISTS {CENSO_DATABASE};\"))\n",
    "    conn.execute(text(f\"CREATE DATABASE {CENSO_DATABASE};\"))    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9363de9-0448-4cef-87cc-84107c3cc933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backref(back_populates: str) -> Mapped[Any]:\n",
    "    return relationship(back_populates=back_populates)\n",
    "\n",
    "\n",
    "def childOf(back_populates: str) -> Mapped[Any]:\n",
    "    return relationship(\n",
    "        back_populates=back_populates,\n",
    "        cascade=\"all, delete-orphan\",\n",
    "    )\n",
    "\n",
    "def digits(name:str) -> CheckConstraint:\n",
    "    return constraint(\"id ~ '^[0-9]+$'\", name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28f60461-1951-4e40-a91b-a0665447ae9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Households:\n",
    "    def __init__(self, urban: int, rural: int) -> None:\n",
    "        self.urban = urban\n",
    "        self.rural = rural\n",
    "\n",
    "    def __composite_values__(self) -> tuple[int, ...]:\n",
    "        return (self.urban, self.rural)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, Households) and \\\n",
    "               other.urban == self.urban and \\\n",
    "               other.rural == self.rural\n",
    "\n",
    "    def __ne__(self, other):\n",
    "        return not self.__eq__(other)\n",
    "        \n",
    "    @hybrid_property\n",
    "    def total(self):\n",
    "        \"\"\"Python-side property for total households.\"\"\"\n",
    "        return self.urban + self.rural\n",
    "\n",
    "    @total.expression\n",
    "    def total(cls):\n",
    "        \"\"\"SQL-side expression for querying total households.\"\"\"\n",
    "        return cls.urban + cls.rural\n",
    "\n",
    "class Coordinate:\n",
    "    \"\"\"\n",
    "    A geographic coordinate point with longitude (lon) and latitude (lat) components.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, longitude: float, latitude: float) -> None:\n",
    "        self.longitude = longitude\n",
    "        self.latitude = latitude\n",
    "\n",
    "    @property\n",
    "    def longitude(self) -> float:\n",
    "        return self._longitude\n",
    "\n",
    "    @longitude.setter\n",
    "    def longitude(self, value: float) -> None:\n",
    "        if not (-180 <= value <= 180):\n",
    "            raise ValueError(\n",
    "                f\"Longitude must be between -180 and 180 degrees, got {value}\"\n",
    "            )\n",
    "        self._longitude = value\n",
    "\n",
    "    @property\n",
    "    def latitude(self) -> float:\n",
    "        return self._latitude\n",
    "\n",
    "    @latitude.setter\n",
    "    def latitude(self, value: float) -> None:\n",
    "        if not (-90 <= value <= 90):\n",
    "            raise ValueError(\n",
    "                f\"Latitude must be between -90 and 90 degrees, got {value}\"\n",
    "            )\n",
    "        self._latitude = value\n",
    "\n",
    "    def __composite_values__(self) -> tuple[float, ...]:\n",
    "        return (self.longitude, self.latitude)\n",
    "\n",
    "    def __eq__(self, other: \"Coordinate\") -> bool:\n",
    "        return isinstance(other, Coordinate) and \\\n",
    "               other.longitude == self.longitude and \\\n",
    "               other.latitude == self.latitude\n",
    "        \n",
    "    def __ne__ (self, other: \"Coordinate\") -> bool:\n",
    "        return not self.__eq__(other)\n",
    "\n",
    "    @hybrid_method\n",
    "    def distance(\n",
    "        self,\n",
    "        other: \"Coordinate\",\n",
    "        metric: str = 'euclidean'\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Calculate distance to another coordinate.\n",
    "        \n",
    "        Args:\n",
    "            other: Coordinate instance\n",
    "            metric: 'euclidean' or 'manhattan'\n",
    "        \n",
    "        Returns:\n",
    "            Distance between coordinates\n",
    "        \"\"\"\n",
    "        x = self.longitude - other.longitude\n",
    "        y = self.latitude - other.latitude\n",
    "        match metric:\n",
    "            case 'euclidean':\n",
    "                return sqrt(x**2 + y**2)\n",
    "            case 'manhattan':\n",
    "                return abs(x) + abs(y)\n",
    "            case _:\n",
    "                raise ValueError(\n",
    "                    \"Metric must be 'euclidean' or 'manhattan'\"\n",
    "                )\n",
    "\n",
    "    @distance.expression\n",
    "    def distance(\n",
    "        cls,\n",
    "        other_lon: float,\n",
    "        other_lat: float,\n",
    "        metric: str = 'euclidean'\n",
    "    ):\n",
    "        x = cls.longitude - other_lon\n",
    "        y = cls.latitude - other_lat\n",
    "        \n",
    "        match metric:\n",
    "            case 'euclidean':\n",
    "                return func.sqrt(y * x + y * y)\n",
    "            case 'manhattan':\n",
    "                return func.abs(x) + func.abs(y)\n",
    "            case _:\n",
    "                raise ValueError(\n",
    "                    \"Metric must be 'euclidean' or 'manhattan'\"\n",
    "                )\n",
    "\n",
    "\n",
    "class Biomes:\n",
    "    default: dict[str, float] = {\n",
    "        biome: 0.0 for biome in [\n",
    "            'amazon_rainforest',\n",
    "            'atlantic_forest',\n",
    "            'caatinga',\n",
    "            'cerrado',\n",
    "            'pantanal',\n",
    "            'pampas'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        self.distribution = kwargs\n",
    "\n",
    "    def __composite_values__(self) -> tuple[float, ...]:\n",
    "        return tuple(getattr(self, biome) for biome in self.default.keys())\n",
    "\n",
    "    def __eq__(self, other: \"Biomes\") -> bool:\n",
    "        return self.__composite_values__() == other.__composite_values__()\n",
    "\n",
    "    def __ne__(self, other: \"Biomes\") -> bool:\n",
    "        return not self.__eq__(other)\n",
    "\n",
    "    @property\n",
    "    def distribution(self) -> dict[str, float]:\n",
    "        return {biome: getattr(self, biome) for biome in self.default.keys()}\n",
    "\n",
    "\n",
    "    @distribution.setter\n",
    "    def distribution(self, values: dict[str, float]) -> None:\n",
    "        merged_values = {**self.default, **values}\n",
    "\n",
    "        # Validation\n",
    "        invalid_keys = set(merged_values.keys()) - set(self.default.keys())\n",
    "        if invalid_keys:\n",
    "            raise ValueError(\n",
    "                f\"Invalid biome types: {invalid_keys}. Valid types are: {list(self.default.keys())}\"\n",
    "            )\n",
    "        total = sum(merged_values.values())\n",
    "        if not (99.9 <= total <= 100.1):\n",
    "            raise ValueError(\n",
    "                f\"Invalid biome distribution, totalling {total:.1f}%\"\n",
    "            )\n",
    "        self._distribution = merged_values\n",
    "\n",
    "        for biome_type, value in merged_values.items():\n",
    "            setattr(self, biome_type, value)\n",
    "\n",
    "    @property\n",
    "    def total(self) -> float:\n",
    "        return sum(getattr(self, biome) for biome in self.default.keys())\n",
    "\n",
    "    @classmethod\n",
    "    def toList(cls) -> list[str]:\n",
    "        return list(cls.default.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "198f174f-0553-4092-8f21-9d1f041c0ab4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConstraintColumnNotFoundError",
     "evalue": "Can't create UniqueConstraint on table 'states': no column named 'coordinates' is present.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/v5s2p5acimzx3109b1h5irz38bxghrnv-python3-3.12.11-env/lib/python3.12/site-packages/sqlalchemy/sql/schema.py:4336\u001b[39m, in \u001b[36mColumnCollectionMixin._col_expressions\u001b[39m\u001b[34m(self, parent)\u001b[39m\n\u001b[32m   4334\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   4335\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m-> \u001b[39m\u001b[32m4336\u001b[39m         \u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m col\n\u001b[32m   4337\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pending_colargs\n\u001b[32m   4338\u001b[39m     ]\n\u001b[32m   4339\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ke:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/v5s2p5acimzx3109b1h5irz38bxghrnv-python3-3.12.11-env/lib/python3.12/site-packages/sqlalchemy/sql/base.py:1608\u001b[39m, in \u001b[36mColumnCollection.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1607\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1608\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   1609\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[31mKeyError\u001b[39m: 'coordinates'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mConstraintColumnNotFoundError\u001b[39m             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 90\u001b[39m\n\u001b[32m     82\u001b[39m             \u001b[38;5;28;01mcase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01m_\u001b[39;00m:\n\u001b[32m     83\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     84\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33mMetric must be \u001b[39m\u001b[33m'\u001b[39m\u001b[33meuclidean\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mmanhattan\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     85\u001b[39m                 )\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mState\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mLocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43m__tablename__\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Attributes\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/v5s2p5acimzx3109b1h5irz38bxghrnv-python3-3.12.11-env/lib/python3.12/site-packages/sqlalchemy/orm/decl_api.py:198\u001b[39m, in \u001b[36mDeclarativeMeta.__init__\u001b[39m\u001b[34m(cls, classname, bases, dict_, **kw)\u001b[39m\n\u001b[32m    195\u001b[39m         \u001b[38;5;28mcls\u001b[39m._sa_registry = reg\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.\u001b[34m__dict__\u001b[39m.get(\u001b[33m\"\u001b[39m\u001b[33m__abstract__\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[43m_as_declarative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdict_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[38;5;28mtype\u001b[39m.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, classname, bases, dict_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/v5s2p5acimzx3109b1h5irz38bxghrnv-python3-3.12.11-env/lib/python3.12/site-packages/sqlalchemy/orm/decl_base.py:244\u001b[39m, in \u001b[36m_as_declarative\u001b[39m\u001b[34m(registry, cls, dict_)\u001b[39m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_as_declarative\u001b[39m(\n\u001b[32m    240\u001b[39m     registry: _RegistryType, \u001b[38;5;28mcls\u001b[39m: Type[Any], dict_: _ClassDict\n\u001b[32m    241\u001b[39m ) -> Optional[_MapperConfig]:\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m# declarative scans the class for attributes.  no table or mapper\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# args passed separately.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MapperConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43msetup_mapping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregistry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdict_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/v5s2p5acimzx3109b1h5irz38bxghrnv-python3-3.12.11-env/lib/python3.12/site-packages/sqlalchemy/orm/decl_base.py:325\u001b[39m, in \u001b[36m_MapperConfig.setup_mapping\u001b[39m\u001b[34m(cls, registry, cls_, dict_, table, mapper_kw)\u001b[39m\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _DeferredMapperConfig(\n\u001b[32m    322\u001b[39m         registry, cls_, dict_, table, mapper_kw\n\u001b[32m    323\u001b[39m     )\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ClassScanMapperConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m        \u001b[49m\u001b[43mregistry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcls_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdict_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper_kw\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/v5s2p5acimzx3109b1h5irz38bxghrnv-python3-3.12.11-env/lib/python3.12/site-packages/sqlalchemy/orm/decl_base.py:576\u001b[39m, in \u001b[36m_ClassScanMapperConfig.__init__\u001b[39m\u001b[34m(self, registry, cls_, dict_, table, mapper_kw)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;28mself\u001b[39m._extract_mappable_attributes()\n\u001b[32m    574\u001b[39m \u001b[38;5;28mself\u001b[39m._extract_declared_columns()\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[38;5;28mself\u001b[39m._setup_inheriting_columns(mapper_kw)\n\u001b[32m    580\u001b[39m \u001b[38;5;28mself\u001b[39m._early_mapping(mapper_kw)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/v5s2p5acimzx3109b1h5irz38bxghrnv-python3-3.12.11-env/lib/python3.12/site-packages/sqlalchemy/orm/decl_base.py:1757\u001b[39m, in \u001b[36m_ClassScanMapperConfig._setup_table\u001b[39m\u001b[34m(self, table)\u001b[39m\n\u001b[32m   1749\u001b[39m             table_kw[\u001b[33m\"\u001b[39m\u001b[33mautoload\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1751\u001b[39m         sorted_columns = \u001b[38;5;28msorted\u001b[39m(\n\u001b[32m   1752\u001b[39m             declared_columns,\n\u001b[32m   1753\u001b[39m             key=\u001b[38;5;28;01mlambda\u001b[39;00m c: column_ordering.get(c, \u001b[32m0\u001b[39m),\n\u001b[32m   1754\u001b[39m         )\n\u001b[32m   1755\u001b[39m         table = \u001b[38;5;28mself\u001b[39m.set_cls_attribute(\n\u001b[32m   1756\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m__table__\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1757\u001b[39m             \u001b[43mtable_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1758\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtablename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1759\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_metadata_for_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1760\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43msorted_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1761\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1762\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtable_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1763\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1764\u001b[39m         )\n\u001b[32m   1765\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1766\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m table \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:2\u001b[39m, in \u001b[36m__new__\u001b[39m\u001b[34m(cls, *args, **kw)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/v5s2p5acimzx3109b1h5irz38bxghrnv-python3-3.12.11-env/lib/python3.12/site-packages/sqlalchemy/util/deprecations.py:281\u001b[39m, in \u001b[36mdeprecated_params.<locals>.decorate.<locals>.warned\u001b[39m\u001b[34m(fn, *args, **kwargs)\u001b[39m\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[32m    275\u001b[39m         _warn_with_version(\n\u001b[32m    276\u001b[39m             messages[m],\n\u001b[32m    277\u001b[39m             versions[m],\n\u001b[32m    278\u001b[39m             version_warnings[m],\n\u001b[32m    279\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    280\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/v5s2p5acimzx3109b1h5irz38bxghrnv-python3-3.12.11-env/lib/python3.12/site-packages/sqlalchemy/sql/schema.py:430\u001b[39m, in \u001b[36mTable.__new__\u001b[39m\u001b[34m(cls, *args, **kw)\u001b[39m\n\u001b[32m    423\u001b[39m \u001b[38;5;129m@util\u001b[39m.deprecated_params(\n\u001b[32m    424\u001b[39m     mustexist=(\n\u001b[32m    425\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m1.4\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    428\u001b[39m )\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, *args: Any, **kw: Any) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/v5s2p5acimzx3109b1h5irz38bxghrnv-python3-3.12.11-env/lib/python3.12/site-packages/sqlalchemy/sql/schema.py:484\u001b[39m, in \u001b[36mTable._new\u001b[39m\u001b[34m(cls, *args, **kw)\u001b[39m\n\u001b[32m    482\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m table\n\u001b[32m    483\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_remove_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/v5s2p5acimzx3109b1h5irz38bxghrnv-python3-3.12.11-env/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py:146\u001b[39m, in \u001b[36msafe_reraise.__exit__\u001b[39m\u001b[34m(self, type_, value, traceback)\u001b[39m\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m exc_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value.with_traceback(exc_tb)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    148\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/v5s2p5acimzx3109b1h5irz38bxghrnv-python3-3.12.11-env/lib/python3.12/site-packages/sqlalchemy/sql/schema.py:480\u001b[39m, in \u001b[36mTable._new\u001b[39m\u001b[34m(cls, *args, **kw)\u001b[39m\n\u001b[32m    478\u001b[39m metadata._add_table(name, schema, table)\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m     \u001b[43mtable\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_no_init\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     table.dispatch.after_parent_attach(table, metadata)\n\u001b[32m    482\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m table\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/v5s2p5acimzx3109b1h5irz38bxghrnv-python3-3.12.11-env/lib/python3.12/site-packages/sqlalchemy/sql/schema.py:874\u001b[39m, in \u001b[36mTable.__init__\u001b[39m\u001b[34m(self, name, metadata, schema, quote, quote_schema, autoload_with, autoload_replace, keep_existing, extend_existing, resolve_fks, include_columns, implicit_returning, comment, info, listeners, prefixes, _extend_on, _no_init, *args, **kw)\u001b[39m\n\u001b[32m    862\u001b[39m     \u001b[38;5;28mself\u001b[39m._autoload(\n\u001b[32m    863\u001b[39m         metadata,\n\u001b[32m    864\u001b[39m         autoload_with,\n\u001b[32m   (...)\u001b[39m\u001b[32m    868\u001b[39m         resolve_fks=resolve_fks,\n\u001b[32m    869\u001b[39m     )\n\u001b[32m    871\u001b[39m \u001b[38;5;66;03m# initialize all the column, etc. objects.  done after reflection to\u001b[39;00m\n\u001b[32m    872\u001b[39m \u001b[38;5;66;03m# allow user-overrides\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m874\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_items\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_replacements\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextend_existing\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkeep_existing\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mautoload_with\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43mall_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/v5s2p5acimzx3109b1h5irz38bxghrnv-python3-3.12.11-env/lib/python3.12/site-packages/sqlalchemy/sql/schema.py:234\u001b[39m, in \u001b[36mSchemaItem._init_items\u001b[39m\u001b[34m(self, *args, **kw)\u001b[39m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc.ArgumentError(\n\u001b[32m    230\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mSchemaItem\u001b[39m\u001b[33m'\u001b[39m\u001b[33m object, such as a \u001b[39m\u001b[33m'\u001b[39m\u001b[33mColumn\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    231\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mConstraint\u001b[39m\u001b[33m'\u001b[39m\u001b[33m expected, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    232\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m     \u001b[43mspwd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/v5s2p5acimzx3109b1h5irz38bxghrnv-python3-3.12.11-env/lib/python3.12/site-packages/sqlalchemy/sql/base.py:1324\u001b[39m, in \u001b[36mSchemaEventTarget._set_parent_with_dispatch\u001b[39m\u001b[34m(self, parent, **kw)\u001b[39m\n\u001b[32m   1320\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_set_parent_with_dispatch\u001b[39m(\n\u001b[32m   1321\u001b[39m     \u001b[38;5;28mself\u001b[39m, parent: SchemaEventTarget, **kw: Any\n\u001b[32m   1322\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1323\u001b[39m     \u001b[38;5;28mself\u001b[39m.dispatch.before_parent_attach(\u001b[38;5;28mself\u001b[39m, parent)\n\u001b[32m-> \u001b[39m\u001b[32m1324\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_set_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m     \u001b[38;5;28mself\u001b[39m.dispatch.after_parent_attach(\u001b[38;5;28mself\u001b[39m, parent)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/v5s2p5acimzx3109b1h5irz38bxghrnv-python3-3.12.11-env/lib/python3.12/site-packages/sqlalchemy/sql/schema.py:4410\u001b[39m, in \u001b[36mColumnCollectionConstraint._set_parent\u001b[39m\u001b[34m(self, parent, **kw)\u001b[39m\n\u001b[32m   4408\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(parent, (Column, Table))\n\u001b[32m   4409\u001b[39m Constraint._set_parent(\u001b[38;5;28mself\u001b[39m, parent)\n\u001b[32m-> \u001b[39m\u001b[32m4410\u001b[39m \u001b[43mColumnCollectionMixin\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_set_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/v5s2p5acimzx3109b1h5irz38bxghrnv-python3-3.12.11-env/lib/python3.12/site-packages/sqlalchemy/sql/schema.py:4349\u001b[39m, in \u001b[36mColumnCollectionMixin._set_parent\u001b[39m\u001b[34m(self, parent, **kw)\u001b[39m\n\u001b[32m   4346\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_set_parent\u001b[39m(\u001b[38;5;28mself\u001b[39m, parent: SchemaEventTarget, **kw: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4347\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(parent, (Table, Column))\n\u001b[32m-> \u001b[39m\u001b[32m4349\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_col_expressions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   4350\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4351\u001b[39m             \u001b[38;5;28mself\u001b[39m._columns.add(col)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/v5s2p5acimzx3109b1h5irz38bxghrnv-python3-3.12.11-env/lib/python3.12/site-packages/sqlalchemy/sql/schema.py:4340\u001b[39m, in \u001b[36mColumnCollectionMixin._col_expressions\u001b[39m\u001b[34m(self, parent)\u001b[39m\n\u001b[32m   4335\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m   4336\u001b[39m         parent.c[col] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m col\n\u001b[32m   4337\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pending_colargs\n\u001b[32m   4338\u001b[39m     ]\n\u001b[32m   4339\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ke:\n\u001b[32m-> \u001b[39m\u001b[32m4340\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc.ConstraintColumnNotFoundError(\n\u001b[32m   4341\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt create \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4342\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mon table \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent.description\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: no column \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4343\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnamed \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mke.args[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is present.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4344\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mke\u001b[39;00m\n",
      "\u001b[31mConstraintColumnNotFoundError\u001b[39m: Can't create UniqueConstraint on table 'states': no column named 'coordinates' is present."
     ]
    }
   ],
   "source": [
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "class Region(Base):\n",
    "    __tablename__: str = \"regions\"\n",
    "\n",
    "    # Attributes\n",
    "    id: Mapped[str] = column(\n",
    "        String(1),\n",
    "        digits(\"ck_region_id\"),\n",
    "        primary_key=True\n",
    "    )\n",
    "    name: Mapped[str] = column(unique=True)\n",
    "\n",
    "    # Relationships\n",
    "    states: Mapped[list[\"State\"]] = childOf('region')\n",
    "\n",
    "class Location(Base):\n",
    "    __abstract__: bool = True\n",
    "\n",
    "    @dataclass\n",
    "    class Coordinates:\n",
    "        longitude: float\n",
    "        latitude: float\n",
    "\n",
    "    @declared_attr\n",
    "    def coordinates(cls) -> Mapped[Coordinates]:\n",
    "        \"\"\"\n",
    "        This method is now called for each subclass (State, City),\n",
    "        generating a new composite object for each.\n",
    "        \"\"\"\n",
    "        return composite(\n",
    "            mapped_column(attr.name) for attr in fields(cls.Coordinates)\n",
    "        )\n",
    "\n",
    "    @validates(\"coordinates.longitude\")\n",
    "    def validate_longitude(self, key, value: float) -> float:\n",
    "        \"\"\"Ensures longitude is within the valid range of -180 to 180.\"\"\"\n",
    "        if not (-180 <= value <= 180):\n",
    "            raise ValueError(f\"Longitude must be between -180 and 180, but got {value}\")\n",
    "        return value\n",
    "\n",
    "    @validates(\"coordinates.latitude\")\n",
    "    def validate_latitude(self, key, value: float) -> float:\n",
    "        \"\"\"Ensures latitude is within the valid range of -90 to 90.\"\"\"\n",
    "        if not (-90 <= value <= 90):\n",
    "            raise ValueError(f\"Latitude must be between -90 and 90, but got {value}\")\n",
    "        return value\n",
    "\n",
    "    @hybrid_method\n",
    "    def distance(\n",
    "        self,\n",
    "        other: \"Location\",\n",
    "        metric: str = 'euclidean'\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Calculate distance to another coordinate.\n",
    "        \n",
    "        Args:\n",
    "            other: Coordinate instance\n",
    "            metric: 'euclidean' or 'manhattan'\n",
    "        \n",
    "        Returns:\n",
    "            Distance between coordinates\n",
    "        \"\"\"\n",
    "        x = self.coordinates.longitude - other.coordinates.longitude\n",
    "        y = self.coordinates.latitude - other.coordinates.latitude\n",
    "        match metric:\n",
    "            case 'euclidean':\n",
    "                return sqrt(x**2 + y**2)\n",
    "            case 'manhattan':\n",
    "                return abs(x) + abs(y)\n",
    "            case _:\n",
    "                raise ValueError(\n",
    "                    \"Metric must be 'euclidean' or 'manhattan'\"\n",
    "                )\n",
    "\n",
    "    @distance.expression\n",
    "    def distance(\n",
    "        cls,\n",
    "        other_lon: float,\n",
    "        other_lat: float,\n",
    "        metric: str = 'euclidean'\n",
    "    ):\n",
    "        x = cls.coordinates.longitude - other_lon\n",
    "        y = cls.coordinates.latitude - other_lat\n",
    "        \n",
    "        match metric:\n",
    "            case 'euclidean':\n",
    "                return func.sqrt(y * x + y * y)\n",
    "            case 'manhattan':\n",
    "                return func.abs(x) + func.abs(y)\n",
    "            case _:\n",
    "                raise ValueError(\n",
    "                    \"Metric must be 'euclidean' or 'manhattan'\"\n",
    "                )\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "class State(Location):\n",
    "    __tablename__: str = \"states\"\n",
    "\n",
    "    # Attributes\n",
    "    id: Mapped[str] = column(String(1), digits(\"ck_state_id\"))\n",
    "    name: Mapped[str] = column(unique=True)\n",
    "    uf: Mapped[str] = column(String(2), unique=True)\n",
    "    area: Mapped[float]\n",
    "    biome_distribution: Mapped[Biomes] = composite(\n",
    "        *[column(biome, Float, default=0.0) for biome in Biomes.toList()]\n",
    "    )\n",
    "\n",
    "    # Foreign keys\n",
    "    region_id: Mapped[str] = column(fk(\"regions.id\"))\n",
    "\n",
    "    # Relationships\n",
    "    region: Mapped[\"Region\"] = backref(\"states\")\n",
    "    cities: Mapped[list[\"City\"]] = childOf(\"state\")\n",
    "\n",
    "    __table_args__: tuple[pkc, unique,] = (\n",
    "        pkc(\"region_id\", \"id\"),\n",
    "        unique('longitude', 'latitude', name='uq_state_location'),\n",
    "    )\n",
    "\n",
    "\n",
    "class City(Location):\n",
    "    __tablename__: str = \"cities\"\n",
    "\n",
    "    # Attributes\n",
    "    id: Mapped[str] = column(String(5), digits(\"ck_city_id\"))\n",
    "    name: Mapped[str]\n",
    "    is_capital: Mapped[bool] = column(default=False, server_default='false')\n",
    "    ddd: Mapped[str] = column(String(2), digits(\"ck_city_ddd\"))\n",
    "    households: Mapped[Households] = composite(\n",
    "        column(\"urban\", Integer),\n",
    "        column(\"rural\", Integer)\n",
    "    )\n",
    "    population_race: Mapped[dict | None] = column(JSON)\n",
    "    population_education: Mapped[dict | None] = column(JSON)\n",
    "    \n",
    "\n",
    "    # Foreign keys\n",
    "    timezone_name: Mapped[str] = column(fk(\"timezones.name\"))\n",
    "    region_id: Mapped[str]\n",
    "    state_id: Mapped[str]\n",
    "\n",
    "    # Relationships\n",
    "    timezone: Mapped[\"Timezone\"] = backref(\"cities\")\n",
    "    state: Mapped[\"State\"] = backref(\"cities\")\n",
    "\n",
    "    @hybrid_property\n",
    "    def ibge_code(self) -> str:\n",
    "        \"\"\"Python-side property to get the IBGE code.\"\"\"\n",
    "        return self.region_id + self.state_id + self.id\n",
    "\n",
    "    @ibge_code.expression\n",
    "    def ibge_code(cls):\n",
    "        \"\"\"SQL-side expression for querying.\"\"\"\n",
    "        return cast(\n",
    "            func.concat(\n",
    "                # Join to State to get the region ID\n",
    "                cast(cls.region_id, String),\n",
    "                cast(cls.state_id, String),\n",
    "                cast(cls.id, String)\n",
    "            ),\n",
    "            BigInteger\n",
    "        )\n",
    "\n",
    "    __table_args__: tuple[pkc, fkc, unique, ] = (\n",
    "        pkc(\"region_id\", \"state_id\", \"id\"),\n",
    "        fkc(\n",
    "            ['region_id', 'state_id'],\n",
    "            ['states.region_id', 'states.id'],\n",
    "            name='fk_region_composite'\n",
    "        ),\n",
    "        unique(\"longitude\", \"latitude\", name=\"uq_city_location\"),\n",
    "        Index(\n",
    "            'state_capitals_index',\n",
    "            'region_id', 'state_id',\n",
    "            postgresql_where=text('is_capital'),\n",
    "            unique=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "class Timezone(Base):\n",
    "    __tablename__: str = \"timezones\"\n",
    "\n",
    "    # Attributes\n",
    "    name: Mapped[str] = column(primary_key=True)\n",
    "    utc_offset: Mapped[int]\n",
    "\n",
    "    # Relationships\n",
    "    cities: Mapped[list[\"City\"]] = backref('timezone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a4df84-58fa-458a-a1a3-e5707bad84c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(URI + CENSO_DATABASE, echo=True)\n",
    "Session = sessionmaker(bind=engine)\n",
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cf92cf-898b-4989-a0a7-438390d5c77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Region Data\n",
    "regions_data = [\"Norte\", \"Nordeste\", \"Sudeste\", \"Sul\", \"Centro-Oeste\"]\n",
    "\n",
    "# 2. Timezone Data\n",
    "timezones_data = [\n",
    "    ('America/Noronha', -2),\n",
    "    ('America/Sao_Paulo', -3),\n",
    "    ('America/Brasilia', -3),\n",
    "    ('America/Recife', -3),\n",
    "    ('America/Porto_Velho', -4),\n",
    "    ('America/Manaus', -4),\n",
    "    ('America/Rio_Branco', -5),\n",
    "]\n",
    "\n",
    "with Session() as session:\n",
    "    # Populate the Regions table\n",
    "    regions = [\n",
    "        Region(id=str(idx), name=name) for idx, name in enumerate(regions_data, start=1)\n",
    "    ]\n",
    "    session.add_all(regions)\n",
    "\n",
    "    # Populate the Timezones table\n",
    "    timezones = [Timezone(name=name, utc_offset=offset) for name, offset in timezones_data]\n",
    "    session.add_all(timezones)\n",
    "\n",
    "    # Commit the changes to the database\n",
    "    session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4996ad7-1a1e-450d-8f42-f9fa0b1d128c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(**FILES['states'])\n",
    "df.fillna(0.0, inplace=True)\n",
    "BIOME_COLUMN_MAPPING = {\n",
    "    'Amazônia': 'amazon_rainforest',\n",
    "    'Mata Atlântica': 'atlantic_forest',\n",
    "    'Caatinga': 'caatinga',\n",
    "    'Cerrado': 'cerrado',\n",
    "    'Pantanal': 'pantanal',\n",
    "    'Pampa': 'pampas'\n",
    "}\n",
    "csv_biome_cols = df.columns[8:].tolist()\n",
    "\n",
    "states_to_insert = []\n",
    "for _, row in df.iterrows():\n",
    "    code_str = str(row['codigouf'])\n",
    "    state_data = {\n",
    "        'region_id': code_str[0],\n",
    "        'id': code_str[1],\n",
    "        'uf': row['uf'],\n",
    "        'name': row['estado'],\n",
    "        'longitude': row['long'],\n",
    "        'latitude': row['lat'],\n",
    "        'area': row['Area']\n",
    "    }\n",
    "\n",
    "    for csv_col in csv_biome_cols:\n",
    "        model_attr = BIOME_COLUMN_MAPPING.get(csv_col)\n",
    "        state_data[model_attr] = row[csv_col]\n",
    "\n",
    "    states_to_insert.append(state_data)\n",
    "\n",
    "\n",
    "with Session() as session:\n",
    "    session.bulk_insert_mappings(State, states_to_insert)\n",
    "    session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4883486c-e36f-49c0-b296-569f467c39bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_city_uf_key(name: str, uf: str) -> str:\n",
    "    \"\"\"Creates a standardized, URL-friendly key from a city name and UF.\"\"\"\n",
    "    # Normalize (remove accents), convert to lowercase, replace spaces with hyphens\n",
    "    normalized_name = ''.join(c for c in unicodedata.normalize('NFD', name) \n",
    "                              if unicodedata.category(c) != 'Mn')\n",
    "    return f\"{normalized_name.lower().replace(' ', '-')}_{uf.lower()}\"\n",
    "\n",
    "def parse_municipio(municipio_str: str) -> tuple[str, str] | None:\n",
    "    \"\"\"Parses a string like 'São Carlos (SP)' into ('São Carlos', 'SP').\"\"\"\n",
    "    match = re.match(r'^(.*?)\\s*\\((.*?)\\)$', municipio_str)\n",
    "    if match:\n",
    "        return match.group(1).strip(), match.group(2).strip()\n",
    "    return None\n",
    "\n",
    "def create_race_json(row: pd.Series) -> dict:\n",
    "    \"\"\"Builds the population_race JSON object from a DataFrame row.\"\"\"\n",
    "    return {\n",
    "        'white': {'male': row[4], 'female': row[5]},\n",
    "        'black': {'male': row[7], 'female': row[8]},\n",
    "        'yellow': {'male': row[9], 'female': row[10]},\n",
    "        'pardo': {'male': row[11], 'female': row[12]},\n",
    "        'indigenous': {'male': row[13], 'female': row[14]},\n",
    "    }\n",
    "\n",
    "def create_education_json(row: pd.Series) -> dict:\n",
    "    \"\"\"Builds the population_education JSON object from a DataFrame row.\"\"\"\n",
    "    return {\n",
    "        'education': {'male': row[3], 'female': row[4]},\n",
    "        'arts_and_humanities': {'male': row[5], 'female': row[6]},\n",
    "        'social_sciences': {'male': row[7], 'female': row[8]},\n",
    "        'administration': {'male': row[9], 'female': row[10]},\n",
    "        'natural_sciences': {'male': row[11], 'female': row[12]},\n",
    "        'information_tecnology': {'male': row[13], 'female': row[14]},\n",
    "        'engineering': {'male': row[15], 'female': row[16]},\n",
    "        'agriculture': {'male': row[17], 'female': row[18]},\n",
    "        'health': {'male': row[19], 'female': row[20]},\n",
    "        'services': {'male': row[21], 'female': row[22]},\n",
    "        'unknown': {'male': row[23], 'female': row[24]},\n",
    "    }\n",
    "\n",
    "with Session() as session:\n",
    "    # Query for the necessary columns from the State table\n",
    "    stmt = select(State.region_id, State.id, State.uf)\n",
    "    results = session.execute(stmt).all()\n",
    "    \n",
    "    # Construct the dictionary by combining region_id and id for the key\n",
    "    state_map = {res.region_id + res.id: res.uf for res in results}\n",
    "\n",
    "    # === PHASE 1: INITIAL LOAD FROM MAIN CSV ===\n",
    "    print(\"Phase 1: Loading initial city data...\")\n",
    "    df_cities = pd.read_csv(**FILES['cities'])\n",
    "    df_cities.fillna({'ddd': 0}, inplace=True) # Handle potential NaN in ddd\n",
    "\n",
    "    cities_to_insert = []\n",
    "    for _, row in df_cities.iterrows():\n",
    "        ibge_code = str(row['codigo_ibge'])\n",
    "        state_code = ibge_code[:2]\n",
    "        \n",
    "        # Create the city_uf_key for future merges\n",
    "        uf = state_map.get(state_code)\n",
    "        \n",
    "        city_data = {\n",
    "            'region_id': ibge_code[0],\n",
    "            'state_id': ibge_code[1],\n",
    "            'id': ibge_code[2:],\n",
    "            'name': row['nome'],\n",
    "            'latitude': row['latitude'],\n",
    "            'longitude': row['longitude'],\n",
    "            'ddd': str(int(row['ddd'])),\n",
    "            'is_capital': bool(row['capital']),\n",
    "            'timezone_name': row['fuso_horario'],\n",
    "            # Add city_uf_key temporarily for in-memory joins\n",
    "            '_city_uf_key': generate_city_uf_key(row['nome'], uf)\n",
    "        }\n",
    "        cities_to_insert.append(city_data)\n",
    "\n",
    "    # Perform the initial bulk insert\n",
    "    session.bulk_insert_mappings(City, cities_to_insert)\n",
    "    session.commit()\n",
    "    print(f\"✅ Inserted {len(cities_to_insert)} base city records.\")\n",
    "\n",
    "    # Create a DataFrame from the inserted data for easy merging\n",
    "    df_inserted = pd.DataFrame(cities_to_insert)\n",
    "\n",
    "    # === PHASE 2: ENRICHMENT ===\n",
    "    \n",
    "    # --- Households Data ---\n",
    "    print(\"\\nPhase 2.1: Enriching with households data...\")\n",
    "    df_house = pd.read_csv(**FILES['households'])\n",
    "    df_house.dropna(subset=['Município'], inplace=True)\n",
    "    \n",
    "    # Create the merge key\n",
    "    parsed_data = df_house['Município'].apply(parse_municipio)\n",
    "    df_house[['name', 'uf']] = pd.DataFrame(parsed_data.tolist(), index=df_house.index)\n",
    "    df_house['_city_uf_key'] = df_house.apply(lambda row: generate_city_uf_key(row['name'], row['uf']), axis=1)\n",
    "\n",
    "    # Merge to get the primary keys\n",
    "    df_merged_house = pd.merge(df_inserted, df_house, on='_city_uf_key')\n",
    "    \n",
    "    households_to_update = df_merged_house[[\n",
    "        'region_id', 'state_id', 'id', 'Urbana', 'Rural'\n",
    "    ]].rename(columns={'Urbana': 'urban', 'Rural': 'rural'}).to_dict('records')\n",
    "\n",
    "    session.bulk_update_mappings(City, households_to_update)\n",
    "    session.commit()\n",
    "    print(f\"✅ Updated {len(households_to_update)} records with household data.\")\n",
    "\n",
    "    # --- Race Data ---\n",
    "    print(\"\\nPhase 2.2: Enriching with population race data...\")\n",
    "    df_race = pd.read_csv(**FILES['race'])\n",
    "    df_race.dropna(subset=[0], inplace=True)\n",
    "\n",
    "    parsed_data_race = df_race[0].apply(parse_municipio)\n",
    "    df_race[['name', 'uf']] = pd.DataFrame(parsed_data_race.tolist(), index=df_race.index)\n",
    "    df_race['_city_uf_key'] = df_race.apply(lambda row: generate_city_uf_key(row['name'], row['uf']), axis=1)\n",
    "    df_race['population_race'] = df_race.apply(create_race_json, axis=1)\n",
    "\n",
    "    df_merged_race = pd.merge(df_inserted, df_race, on='_city_uf_key')\n",
    "\n",
    "    race_to_update = df_merged_race[[\n",
    "        'region_id', 'state_id', 'id', 'population_race'\n",
    "    ]].to_dict('records')\n",
    "\n",
    "    session.bulk_update_mappings(City, race_to_update)\n",
    "    session.commit()\n",
    "    print(f\"✅ Updated {len(race_to_update)} records with race data.\")\n",
    "    \n",
    "    # --- Education Data ---\n",
    "    print(\"\\nPhase 2.3: Enriching with population education data...\")\n",
    "    df_edu = pd.read_csv(**FILES['education'])\n",
    "    df_edu.dropna(subset=[0], inplace=True)\n",
    "\n",
    "    parsed_data_edu = df_edu[0].apply(parse_municipio)\n",
    "    df_edu[['name', 'uf']] = pd.DataFrame(parsed_data_edu.tolist(), index=df_edu.index)\n",
    "    df_edu['_city_uf_key'] = df_edu.apply(lambda row: generate_city_uf_key(row['name'], row['uf']), axis=1)\n",
    "    df_edu['population_education'] = df_edu.apply(create_education_json, axis=1)\n",
    "\n",
    "    df_merged_edu = pd.merge(df_inserted, df_edu, on='_city_uf_key')\n",
    "\n",
    "    edu_to_update = df_merged_edu[[\n",
    "        'region_id', 'state_id', 'id', 'population_education'\n",
    "    ]].to_dict('records')\n",
    "\n",
    "    session.bulk_update_mappings(City, edu_to_update)\n",
    "    session.commit()\n",
    "    print(f\"✅ Updated {len(edu_to_update)} records with education data.\")\n",
    "    \n",
    "    print(\"\\n🎉 All city data loaded and merged successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0e3ca1-0360-4df1-b92b-cde0bd449e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearby_cities(\n",
    "    session: Session,\n",
    "    city_name: str,\n",
    "    uf_code: str,\n",
    "    count: int,\n",
    "    distance_metric: str\n",
    ") -> list[City] | None:\n",
    "    \"\"\"\n",
    "    Finds a specified number of cities closest to a target city.\n",
    "\n",
    "    Args:\n",
    "        session: The SQLAlchemy session for database queries.\n",
    "        city_name: The name of the target city (e.g., 'São Carlos').\n",
    "        uf_code: The two-letter state code of the target city (e.g., 'SP').\n",
    "        count: The number of nearby cities to return.\n",
    "        distance_metric: The distance metric ('manhattan' or 'euclidean').\n",
    "\n",
    "    Returns:\n",
    "        A list of City ORM objects, or None if the target city is not found.\n",
    "    \"\"\"\n",
    "    # Step 1: Find the target city's coordinates and IBGE code\n",
    "    print(f\"Finding coordinates for {city_name}, {uf_code}...\")\n",
    "    stmt = select(City.longitude, City.latitude).join_from(City, State).where(\n",
    "        City.name == city_name,\n",
    "        State.uf == uf_code\n",
    "    )\n",
    "    target = session.execute(stmt).first()\n",
    "    \n",
    "    if not target:\n",
    "        print(f\"Error: {city_name}, {uf_code} not found in the database.\")\n",
    "        return None\n",
    "        \n",
    "    target_lon = target.longitude\n",
    "    target_lat = target.latitude\n",
    "    print(f\"Found {city_name} at ({target_lat:.4f}, {target_lon:.4f})\")\n",
    "\n",
    "    # Step 2: Query for the closest cities\n",
    "    print(f\"Querying for the top {count + 1} closest cities using {distance_metric} distance...\")\n",
    "    \n",
    "    # CORRECTED: Call the hybrid method on the City.location attribute\n",
    "    distance_expr = City.distance(\n",
    "        target_lon, target_lat, metric=distance_metric\n",
    "    ).label(\"distance\")\n",
    "    \n",
    "    stmt = select(City).order_by(distance_expr).limit(count + 1)\n",
    "    \n",
    "    all_results = session.execute(stmt).scalars().all()\n",
    "    closest_cities = all_results[1:]\n",
    "    \n",
    "    print(f\"Found {len(closest_cities)} nearby cities:\")\n",
    "    for city in closest_cities:\n",
    "        # Assuming the 'state' relationship is available for eager/lazy loading\n",
    "        print(f\"- {city.name}, {city.state.uf}\")\n",
    "        \n",
    "    return closest_cities\n",
    "\n",
    "def plot_demographic_distributions(\n",
    "    cities: list[City],\n",
    "    language: str = 'en'\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Processes and plots demographic data from a list of City objects, with\n",
    "    support for English ('en') and Portuguese ('pt') languages.\n",
    "    \"\"\"\n",
    "    if not cities:\n",
    "        print(\"No cities provided to plot. Aborting.\")\n",
    "        return\n",
    "\n",
    "    # --- Translation Dictionary ---\n",
    "    TRANSLATIONS = {\n",
    "        'pt': {\n",
    "            'male': 'Homens', 'female': 'Mulheres', 'gender': 'Gênero',\n",
    "            'race_cats': {\n",
    "                'White': 'Branca', 'Black': 'Preta', 'Yellow': 'Amarela',\n",
    "                'Pardo': 'Parda', 'Indigenous': 'Indígena'\n",
    "            },\n",
    "            'edu_cats': {\n",
    "                'Education': 'Educação', 'Arts And Humanities': 'Artes e Humanidades',\n",
    "                'Social Sciences': 'Ciências Sociais', 'Administration': 'Administração',\n",
    "                'Natural Sciences': 'Ciências Naturais', 'Information Tecnology': 'Tecnologia da Informação',\n",
    "                'Engineering': 'Engenharia', 'Agriculture': 'Agricultura',\n",
    "                'Health': 'Saúde', 'Services': 'Serviços', 'Unknown': 'Não especificado'\n",
    "            },\n",
    "            'plot1_title': 'População por Raça e Gênero ({count} Cidades Próximas)',\n",
    "            'plot1_xlabel': 'Categoria de Raça',\n",
    "            'plot2_title': 'População por Área de Formação e Gênero ({count} Cidades Próximas)',\n",
    "            'plot2_xlabel': 'Área de Formação',\n",
    "            'plot3_title': 'Distribuição Total de Homens vs. Mulheres ({count} Cidades Próximas)',\n",
    "            'total_pop': 'População Total', 'group': 'Grupo',\n",
    "            'group_race': 'Raça', 'group_edu': 'Formação'\n",
    "        }\n",
    "    }\n",
    "    # Fallback to English if language is not 'pt'\n",
    "    lang_map = TRANSLATIONS.get(language, {})\n",
    "\n",
    "    # --- Data Processing ---\n",
    "    race_data = []\n",
    "    edu_data = []\n",
    "    for city in cities:\n",
    "        for cat, genders in city.population_race.items():\n",
    "            en_cat = cat.replace('_', ' ').title()\n",
    "            race_data.append({\n",
    "                \"race\": lang_map.get('race_cats', {}).get(en_cat, en_cat),\n",
    "                \"male\": genders.get('male', 0),\n",
    "                \"female\": genders.get('female', 0)\n",
    "            })\n",
    "        for cat, genders in city.population_education.items():\n",
    "            en_cat = cat.replace('_', ' ').title()\n",
    "            edu_data.append({\n",
    "                \"education_field\": lang_map.get('edu_cats', {}).get(en_cat, en_cat),\n",
    "                \"male\": genders.get('male', 0),\n",
    "                \"female\": genders.get('female', 0)\n",
    "            })\n",
    "\n",
    "    df_race = pd.DataFrame(race_data)\n",
    "    df_edu = pd.DataFrame(edu_data)\n",
    "    race_summary = df_race.groupby('race')[['male', 'female']].sum()\n",
    "    edu_summary = df_edu.groupby('education_field')[['male', 'female']].sum()\n",
    "    \n",
    "    # Rename columns for legend translation\n",
    "    if language == 'pt':\n",
    "        race_summary.rename(columns={'male': lang_map['male'], 'female': lang_map['female']}, inplace=True)\n",
    "        edu_summary.rename(columns={'male': lang_map['male'], 'female': lang_map['female']}, inplace=True)\n",
    "\n",
    "    # --- Plot Generation ---\n",
    "    print(\"Gerando gráficos demográficos...\")\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 22))\n",
    "    \n",
    "    # Plot 1: Race\n",
    "    race_summary.plot(kind='bar', stacked=True, ax=axes[0], colormap='viridis')\n",
    "    axes[0].set_title(lang_map.get('plot1_title', 'Pop. by Race').format(count=len(cities)), fontsize=16)\n",
    "    axes[0].set_xlabel(lang_map.get('plot1_xlabel', 'Race Category'), fontsize=12)\n",
    "    axes[0].set_ylabel(lang_map.get('total_pop', 'Total Population'), fontsize=12)\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 2: Education\n",
    "    edu_summary.plot(kind='bar', stacked=True, ax=axes[1], colormap='plasma')\n",
    "    axes[1].set_title(lang_map.get('plot2_title', 'Pop. by Education').format(count=len(cities)), fontsize=16)\n",
    "    axes[1].set_xlabel(lang_map.get('plot2_xlabel', 'Education Field'), fontsize=12)\n",
    "    axes[1].set_ylabel(lang_map.get('total_pop', 'Total Population'), fontsize=12)\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 3: Totals\n",
    "    df_totals = pd.DataFrame({\n",
    "        lang_map.get('group', 'Group'): [lang_map.get('group_race', 'Race')] * 2 + [lang_map.get('group_edu', 'Education')] * 2,\n",
    "        lang_map.get('gender', 'Gender'): [lang_map.get('male', 'Male'), lang_map.get('female', 'Female')] * 2,\n",
    "        lang_map.get('total_pop', 'Total Population'): [df_race['male'].sum(), df_race['female'].sum(), df_edu['male'].sum(), df_edu['female'].sum()]\n",
    "    })\n",
    "    sns.barplot(data=df_totals, x=lang_map.get('group', 'Group'), y=lang_map.get('total_pop', 'Total Population'), \n",
    "                hue=lang_map.get('gender', 'Gender'), ax=axes[2], palette='muted')\n",
    "    axes[2].set_title(lang_map.get('plot3_title', 'Total Gender Dist.').format(count=len(cities)), fontsize=16)\n",
    "    axes[2].set_xlabel('')\n",
    "    axes[2].set_ylabel(lang_map.get('total_pop', 'Total Population'), fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754ae5eb-e6f2-4338-b03e-a80c1f97ee88",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Session() as session:\n",
    "    cities = find_nearby_cities(session, 'São Carlos', 'SP', 10, 'manhattan')\n",
    "    plot_demographic_distributions(cities, 'pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac58e7a-86c3-4fa4-a387-ca92b9b7748b",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
